<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=hugo-theme content="Axiom 0.8.0"><link rel=icon type=image/png sizes=32x32 href=/blog/image/brand/favicon.png><link rel=icon type=image/x-icon href=/blog/favicon.ico><link rel=apple-touch-icon href=/blog/image/brand/icon-1-1.png><link rel=canonical href=https://keithyipkw.github.io/blog/no_skip_smm/><link rel=preload as=style href="/blog/bundle.css?v=1661093204" media=all><link rel=stylesheet href="/blog/bundle.css?v=1661093204" media=all><style>.cdata pre{background-color:#1f2937;color:#e5e7eb}.cdata :not(pre)>code{background-color:#f3f4f6;color:#7c3aed}.chroma .err{background-color:#991b1b;color:#fecaca}.chroma .hl{background-color:#374151}.chroma .ln{color:#9ca3af}.chroma .k,.chroma .kc,.chroma .kd,.chroma .kn,.chroma .kp,.chroma .kr{color:#60a5fa}.chroma .kt{color:#a78bfa}.chroma .na,.chroma .nb{color:#fbbf24}.chroma .nc{color:#f87171}.chroma .no{color:#34d399}.chroma .nd{color:#f87171}.chroma .ne{color:#f87171}.chroma .nf{color:#fbbf24}.chroma .nt{color:#f87171}.chroma .l{color:#a78bfa}.chroma .dl,.chroma .ld,.chroma .s,.chroma .s2,.chroma .sa,.chroma .sb,.chroma .sc,.chroma .sd{color:#34d399}.chroma .se{color:#9ca3af}.chroma .s1,.chroma .sh,.chroma .si,.chroma .sr,.chroma .ss,.chroma .sx{color:#34d399}.chroma .il,.chroma .m,.chroma .mb,.chroma .mf,.chroma .mh,.chroma .mi,.chroma .mo{color:#a78bfa}.chroma .o,.chroma .ow{color:#93c5fd}.chroma .c,.chroma .c1,.chroma .ch,.chroma .cm,.chroma .cp,.chroma .cpf,.chroma .cs,.chroma .p{color:#9ca3af}.chroma .ge{font-style:italic}.chroma .gs{font-weight:700}</style><title>How Difficult is 1000 Endless Expert Levels Without Skipping in Super Mario Marker 2? - STEM</title><meta property="og:title" content="How Difficult is 1000 Endless Expert Levels Without Skipping in Super Mario Marker 2?"><meta property="og:site_name" content="STEM"><meta property="og:url" content="https://keithyipkw.github.io/blog/no_skip_smm/"><link rel=image_src href=https://keithyipkw.github.io/blog/><meta property="og:image" content="https://keithyipkw.github.io/blog/"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="og:type" content="article"><meta property="og:locale" content="en_us"><meta property="og:description" content="Mario games are fun and challenging. Nintendo has been spending a great deal of effect to design and implement them. In 2015, they released Super Mario Maker. The community could finally legally make and share their levels. With their creativity and dedication, many fun levels which Nintendo would not make were born."><meta name=description content="Mario games are fun and challenging. Nintendo has been spending a great deal of effect to design and implement them. In 2015, they released Super Mario Maker. The community could finally legally make and share their levels. With their creativity and dedication, many fun levels which Nintendo would not make were born."><meta property="og:updated_time" content="2022-03-14T14:20:00Z"><meta property="fb:app_id" content><meta name=author content="Keith Yip"><meta property="article:author" content="https://keithyipkw.github.io/blog/"><meta property="article:published_time" content="2022-01-17T13:03:00Z"><meta property="article:modified_time" content="2022-03-14T14:20:00Z"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"How Difficult is 1000 Endless Expert Levels Without Skipping in Super Mario Marker 2?","alternativeHeadline":"Mario games are fun and challenging. Nintendo has been spending a great deal of effect to design and implement them. In 2015, they released Super Mario Maker. The community could finally legally make and share their levels. With their creativity and dedication, many fun levels which Nintendo would not make were born.","url":"https://keithyipkw.github.io/blog/no_skip_smm/","image":"https://keithyipkw.github.io/blog/","mainEntityOfPage":{"@type":"WebPage","@id":"https://keithyipkw.github.io/blog/no_skip_smm/"},"description":"Mario games are fun and challenging. Nintendo has been spending a great deal of effect to design and implement them. In 2015, they released Super Mario Maker. The community could finally legally make and share their levels. With their creativity and dedication, many fun levels which Nintendo would not make were born.","author":{"@type":"Person","name":"Keith Yip"},"publisher":{"@type":"Organization","name":"STEM","logo":{"@type":"ImageObject","url":"https://keithyipkw.github.io/blog/image/brand/icon-1-1.png"}},"datePublished":"2022-01-17T13:03:00Z","dateModified":"2022-03-14T14:20:00Z","articleBody":"\u003cp\u003eMario games are fun and challenging. Nintendo has been spending a great deal of effect to design and implement them. In 2015, they released Super Mario Maker. The community could finally legally make and share their levels. With their creativity and dedication, many fun levels which Nintendo would not make were born. Unsurprisingly, the community also made loads of trash levels. Some were only by-products of learning to be great level makers, but the others were purposely built to torment players. In 2019, Nintendo released a sequel to the game Super Mario Marker 2. There has been a new game mode, endless challenge.\u003c/p\u003e\n\u003ch1 id=\"no-skip-endless-challenge\"\u003eNo-skip Endless Challenge\u003c/h1\u003e\n\u003cp\u003eIn the endless challenge, you try to complete as many levels the game throws at you as possible. You pick a difficulty and have a certain number of lives to begin. For expert difficulty, you have 15 lives at the start.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eDifficulty\u003c/th\u003e\n\u003cth\u003eNumber of Starting Lives\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eEasy\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eNormal\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eExpert\u003c/td\u003e\n\u003ctd\u003e15\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSuper Expert\u003c/td\u003e\n\u003ctd\u003e30\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eEach time you die on a level, you lose a life. When you have no remaining lives, the run ends. There are multiple ways to gain lives, but you can only redeem them after clearing a level. If there is a flagpole at the end of a level and you touch the top, you receive a life. Similar to other Mario games, collecting a green mushroom or 100 coins grant you a life too. Besides, you can stomp on multiple enemies without touching the ground or using a shell to kill them consecutively to gain lives as well. You can only redeem as most 3 lives in a level because the game limit that to prevent you to farm lives. A generous level maker will unconditionally give you 3 lives or loads of coins. Conversely, an inexperienced or mean one will give you nothing and even block you from reaching the top of the flagpole to gain a life. In addition to the life gain limit in a level, you can have maximally 99 lives.\u003c/p\u003e\n\u003cp\u003eThe game randomly chooses a level made by the community according to the difficulty as your next level. The exact formula of how the game determines the difficulty of a level is not publically known. It is however strongly correlated to the level clear rate ($ \\frac{\\text{#clears}}{\\text{#trials}} $), player clear rate ($ \\frac{\\text{#winning players}}{\\text{#trying players}} $), and the level maker's number of upload trials. For expert difficulty, you should expect to occasionally encounter a level with a clear rate as low as 2%. For such a level, the community wins 1 out of 50 trials. Very rarely, you will encounter a level with a difficulty like the super expert because the formula does not rate the level accurately enough. It seems to happen more often on new levels which have more uncertainty.\u003c/p\u003e\n\u003cp\u003eThe game allows you to skip levels so you will have a more enjoyable experience in the endless challenge. Your run will not immediately end because you encounter a level poorly made, requiring some skills or knowledge you do not have. However, what hardcore gamers want is the exact opposite. Skipping makes the challenge too easy, so they handicap themselves by forbidding it.\u003c/p\u003e\n\u003cp\u003eYour life gain on each level needs to be at least on par with your loss on average to keep a run going in the long term. If you are an average player having only a clear rate of a few percent on an expert level, you virtually have no hope of passing 1000 levels, not even 10 levels. Besides a high clear rate, you also need a lower variance in your life change so you can resist a longer streak of bad levels.\u003c/p\u003e\n\u003ch1 id=\"theory\"\u003eTheory\u003c/h1\u003e\n\u003cp\u003eThe most reliable and straightforward way to estimate the difficulty of the no-skip endless challenge is to use an average success rate of many runs. It is the same as estimating the fairness of a coin. We can use the \u003ca href=\"https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval\"\u003ebinomial proportion\u003c/a\u003e to obtain the uncertainty in terms of the \u003ca href=\"https://en.wikipedia.org/wiki/Confidence_interval\"\u003econfidence interval\u003c/a\u003e. A confidence interval is an interval that covers the true parameter with a certain amount of long-run frequency for (hypothetically) repeatedly random sampling. The sampling process works well for poorly performed players whose success rates are close to 0 because their runs do not last long. It is not the case for skilled players. A run may take many hours. For example, with an average of 3 minutes per level, a successful run will take 50 hours to complete. Without many runs, the uncertainty is large, so the overall confidence intervals are too. For example, the 99% confidence interval of a win and a loss is [6.2%, 93.8%]. Therefore, It is worth putting efforts to squeeze out more information from the existing data to shrink the uncertainty. We will model the problem as a Markov process and use the bootstrap to infer the success rate and the confidence intervals. Generally, the estimated success rate should be closer to the true value for all possible true values and samples. Similarly, the overall confidence intervals should be tighter.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/example_life_changes.png\"\r\nsrcset=\"/blog/image/no_skip_smm/example_life_changes.png 1x,/blog/image/no_skip_smm/example_life_changes_2x.png 2x\"\r\nalt=\"An example of life changes after beating each of the 1000 levels. The sample success rate is about 50%.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eAn example of life changes after beating each of the 1000 levels. The sample success rate is about 50%.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/example_cdf_1.png\"\r\nsrcset=\"/blog/image/no_skip_smm/example_cdf_1.png 1x,/blog/image/no_skip_smm/example_cdf_1_2x.png 2x\"\r\nalt=\"The cumulative probability functions calculated by the two methods. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on the distribution of the life changes above. The other one calculated by the binomial proportion is based on one win and one loss.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe cumulative probability functions calculated by the two methods. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on the distribution of the life changes above. The other one calculated by the binomial proportion is based on one win and one loss.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/example_cdf_2.png\"\r\nsrcset=\"/blog/image/no_skip_smm/example_cdf_2.png 1x,/blog/image/no_skip_smm/example_cdf_2_2x.png 2x\"\r\nalt=\"The cumulative probability functions calculated by the two methods with doubling the samples. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on doubling the distribution of the life changes above. The other one calculated by the binomial proportion is based on two wins and two losses.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe cumulative probability functions calculated by the two methods with doubling the samples. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on doubling the distribution of the life changes above. The other one calculated by the binomial proportion is based on two wins and two losses.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eWe begin by identifying the essence of the process. At the start of a run, a player has 15 lives. The game randomly draws a level as the next level. After playing a level, the number of life may change with an amount ranging from losing all their lives to gaining 3 lives. If the player loses all their lives, the run ends. If the player loses all their lives, the run ends. Conversely, if they gain so many lives that the total exceeds 99, it will still be 99. The number of life and the change are sufficient to describe the player state and a level, respectively.\u003c/p\u003e\n\u003cp\u003eBecause of the game being closed source, Nintendo not publishing any detail, and insufficient samples, we need to make a few assumptions and simplifications:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe game draws a level independent of the number of lives.\u003c/li\u003e\n\u003cli\u003eThe game draws a level independent of the completed levels.\u003c/li\u003e\n\u003cli\u003eThe player's performance is independent of the previous levels.\u003c/li\u003e\n\u003cli\u003eThe player performs practically the same when their number of lives is from 1 to 96 or 97 to 99.\u003c/li\u003e\n\u003cli\u003eThe difficulty of the levels is approximately the same across different runs.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThere has been no information about the independence in the first assumption. Considering that it seems to be and tight budgets are common in the game industry, it is reasonable to assume that Nintendo chose the simplest implementation. The second assumption is at least a close approximation of the real mechanism. There are 20 million levels\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e as of September 2020. Even if a small subset has the expert difficulty, there should still be enough levels that drawing 1000 levels with replacements is approximately the same as drawing without replacements. Thus, we do not need to worry about which case it is. We can simply model it as drawing with replacements. The remaining assumptions are to reduce the sample requirements. Otherwise, we will need a more sophisticated model or much more samples. Careful analysis may disproof some of the assumptions, but it is out of the scope of this article. Regardless, the following will at least serve as an entry point for more sophisticated analysis even if you do not agree with the assumptions.\u003c/p\u003e\n\u003ch2 id=\"modelling-as-a-markov-process\"\u003eModelling as a Markov Process\u003c/h2\u003e\n\u003cp\u003eAn easy way of determining the probability is to simulate many runs. Such a simulation can run 500 times per second on my computer. However, because the variance of the runs is very high, the success rate needs 1 million runs to start converging. If all we need is the expected success rate, it is acceptable to let it run for 33 minutes. But we cannot ignore the potentially significant uncertainty in the collected statistics. We can simulate the sampling process and run the 1 million simulations many times, say 1,000, to approximate the uncertainty. That alone will take 23 days of computational time. To ensure that 1,000 simulations are enough, we should repeat it many times too, say 1,000 times, to verify it. The approach quickly becomes impractical if we want to be thorough. Instead, if we model the problem as a Markov process, we can calculate the success rates analytically 2,500 times per second, not once per 33 minutes. Since the solutions are exact, we eliminate the uncertainty in the simulation.\u003c/p\u003e\n\u003cp\u003eA Markov process is a random process in which the current state determines the future state. In our case, the player's number of lives is the state. By knowing the probability of different life changes after completing a level, we can predict the probabilities of the future number of lives. For example, suppose that a player currently has 10 lives. The probabilities of -1 life, +0 life, and +1 life after finishing the next level are 20%, 30%, and 50%, respectively. The probabilities of having 9, 10, and 11 lives after finishing the next level are 20%, 30%, and 50%, respectively.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/state_change_individual.png\"\r\nsrcset=\"/blog/image/no_skip_smm/state_change_individual.png 1x,/blog/image/no_skip_smm/state_change_individual_2x.png 2x\"\r\nalt=\"The probabilities of the future state can be calculated by considering each mututally exclusive branch separately.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe probabilities of the future state can be calculated by considering each mututally exclusive branch separately.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eFurthermore, different numbers of lives are mutually exclusive states, and so do life changes as mutually exclusive events. By the law of total probability, we can separately consider different states then sum the probabilities for all the outcomes. Referring to the example above but now the probabilities of the current state having 9 and 10 lives are 40% and 60%, respectively, the probability of having 9 lives after finishing the next level is\u003c/p\u003e\n\r\n$$\r\n\\begin{aligned}\r\nP_\\text{next}(\\textcolor{red}{♥}=9) ={} \u0026 P_\\text{current}(\\textcolor{red}{♥}=9) \\times P(\\Delta\\textcolor{red}{♥}=0) \\\\\r\n\u0026 + P_\\text{current}(\\textcolor{red}{♥}=10) \\times P(\\Delta\\textcolor{red}{♥}=-1) \\\\\r\n={} \u0026 40\\% \\times 30\\% + 60\\% \\times 20\\% \\\\\r\n={} \u0026 24\\%\r\n\\end{aligned}\r\n$$\r\n\n\u003cp\u003eSimilarly, the probabilities for 8, 10, and 11 lives are 8%, 38%, and 30%, respectively.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/state_change_combined.png\"\r\nsrcset=\"/blog/image/no_skip_smm/state_change_combined.png 1x,/blog/image/no_skip_smm/state_change_combined_2x.png 2x\"\r\nalt=\"The probability of a future state can be calculated separately then summed.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe probability of a future state can be calculated separately then summed.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eIt is more convenient to organize the calculation in the following matrix form. In addition, the matrix form allows us to leverage accelerated processings by computers during bootstrapping.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/state_change_matrix.png\"\r\nsrcset=\"/blog/image/no_skip_smm/state_change_matrix.png 1x,/blog/image/no_skip_smm/state_change_matrix_2x.png 2x\"\r\nalt=\"The calculation in the matrix form.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe calculation in the matrix form.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eFor the actual problem, let\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ s_k $ be a 100-dimensional row vector for the states after finishing $ k $ levels,\u003c/li\u003e\n\u003cli\u003e$ P_s(k,\\textcolor{red}{♥}=n) $ be the probability of the state after finishing $ k \\in \\set{0,1,2,\\ldots} $ levels and having $ n \\in \\set{0,1,2,\\ldots,99} $ lives,\u003c/li\u003e\n\u003cli\u003e$ T $ be a 100×100 right stochastic matrix,\u003c/li\u003e\n\u003cli\u003e$ P_t(\\textcolor{red}{♥}=n,\\Delta\\textcolor{red}{♥}=m) $ be the probability of transiting from having $ n $ lives to $ (n+m) \\in \\set{0,1,2,\\ldots,99} $ lives.\u003c/li\u003e\n\u003c/ul\u003e\n\r\n$$\r\n\\begin{aligned}\r\ns_k \u0026= \\begin{bmatrix}\r\nP_s(k,\\textcolor{red}{♥}=0) \u0026 P_s(k,\\textcolor{red}{♥}=1) \u0026 \\ldots \u0026 P_s(k,\\textcolor{red}{♥}=99)\r\n\\end{bmatrix} \\\\ \\\\\r\n\r\nT \u0026= \\begin{bmatrix}\r\n1 \u0026 0 \u0026 \\ldots \u0026 0 \\\\\r\nP_t(\\textcolor{red}{♥}=1,\\Delta\\textcolor{red}{♥}=-1) \u0026 P_t(\\textcolor{red}{♥}=1,\\Delta\\textcolor{red}{♥}=0) \u0026 \\ldots \u0026 P_t(\\textcolor{red}{♥}=1,\\Delta\\textcolor{red}{♥}=98) \\\\\r\n\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r\nP_t(\\textcolor{red}{♥}=99,\\Delta\\textcolor{red}{♥}=-99) \u0026 P_t(\\textcolor{red}{♥}=99,\\Delta\\textcolor{red}{♥}=-98) \u0026 \\ldots \u0026 P_t(\\textcolor{red}{♥}=99,\\Delta\\textcolor{red}{♥}=0) \\\\\r\n\\end{bmatrix} \\\\ \\\\\r\n\r\ns_{k+1} \u0026= s_kT \\\\\r\n\r\n\\end{aligned}\r\n$$\r\n\n\u003cp\u003eThe first row in $ T $ is a twist. It allows us to continue to apply the transition to the state even after a run end. This row represents how the \u003ca href=\"https://en.wikipedia.org/wiki/Absorbing_Markov_chain\"\u003eabsorbing state\u003c/a\u003e $ \\textcolor{red}{♥}=0 $ is stuck there.\u003c/p\u003e\n\u003cp\u003eThe other rows in $ T $ are mostly some offset versions of the general life change probabilities. Generally, they are the same as the corresponding transition probabilities for each state, i.e., $ P_t(\\textcolor{red}{♥}=n,\\Delta\\textcolor{red}{♥}=m)=P(\\Delta\\textcolor{red}{♥}=m) $ where $ P(\\Delta\\textcolor{red}{♥}=m) $ is the probability of life change of $ m \\in \\set{0,1,2,\\ldots,99} $ without considering the minimum (0) and maximum (99) numbers of lives. However, the transition probabilities for 0 lives ought to include the life change probabilities causing the number to drop below 0 and vice versa for 99 lives.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/state_change_transition_row.png\"\r\nsrcset=\"/blog/image/no_skip_smm/state_change_transition_row.png 1x,/blog/image/no_skip_smm/state_change_transition_row_2x.png 2x\"\r\nalt=\"In this example, the life change probabilities are for ranging from -3 lives to \u0026#43;1 life. The transition probabilties for the state having 1 life to the state having 0 lives are the sum of the -3, -2 and -1 life probability.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eIn this example, the life change probabilities are for ranging from -3 lives to +1 life. The transition probabilties for the state having 1 life to the state having 0 lives are the sum of the -3, -2 and -1 life probability.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eFormally, the transition probabilities are\u003c/p\u003e\n\r\n$$\r\n\\begin{aligned}\r\n\r\nP_t(\\textcolor{red}{♥}=n,\\Delta\\textcolor{red}{♥}=m) = \\begin{dcases}\r\n\\sum_{q=-\\infty}^{m} P(\\Delta\\textcolor{red}{♥}=q),\u0026 \\text{if } n+m=0\\\\\r\n\\sum_{q=m}^{\\infty} P(\\Delta\\textcolor{red}{♥}=q),\u0026 \\text{if } n+m=99\\\\\r\nP(\\Delta\\textcolor{red}{♥}=m), \u0026 \\text{otherwise}\r\n\\end{dcases}\r\n\r\n\\end{aligned}\r\n$$\r\n\n\u003cp\u003eStarting from the initial state that is 100%, by applying the state change repeatedly, we have the probabilities of the state after finishing any number of levels.\u003c/p\u003e\n\r\n$$\r\n\\begin{aligned}\r\n\r\ns_k \u0026= s_0T^k\r\n\\end{aligned}\r\n$$\r\n\n\u003cp\u003eWe can then calculate the probability of clearing a certain number of levels, and so that of beating the challenge. The probability of beating $ k $ levels is\u003c/p\u003e\n\r\n$$\r\n\\begin{aligned}\r\nP(k) \u0026= 1 - P_s(k,\\textcolor{red}{♥}=0) \\\\\r\n\u0026= \\sum_{\\textcolor{red}{♥}=1}^{99} P_s(k,\\textcolor{red}{♥})\r\n\\end{aligned}\r\n$$\r\n\n\u003ch2 id=\"uncertainty-estimation-by-bootstrapping\"\u003eUncertainty Estimation by Bootstrapping\u003c/h2\u003e\n\u003cp\u003eSampling the frequencies of different life changes allows us to infer the \u003ca href=\"https://en.wikipedia.org/wiki/Statistical_population\"\u003epopulation\u003c/a\u003e, the true probabilities of life changes, and then the parameter, the success rate. It is impossible to know the exact values because it requires the player to play every level infinitely many times to measure the frequencies of different life changes. When we only observe a finite number of samples, there is a random variation between the sample and the population. It is crucial to know the possible variations. Otherwise, we do not know how much the estimation is off. In this article, we will go with the \u003ca href=\"https://en.wikipedia.org/wiki/Frequentist_inference\"\u003efrequentist inference\u003c/a\u003e. For some simple and common problems, there are formulae to calculate the variations in terms of confidence intervals. However, there seems to be none for this problem. We will use a simulation approach instead.\u003c/p\u003e\n\u003cp\u003eBootstrapping is a resampling method based on \u003ca href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\"\u003eMonte Carlo\u003c/a\u003e simulation. The core idea is very simple but yet powerful. We mimic the sampling process of the population by resampling the sample. The sample becomes a known population. With many resamples, our desired \u003ca href=\"https://en.wikipedia.org/wiki/Statistic\"\u003estatistic\u003c/a\u003e, the sample success rate, forms a distribution that we can compare with the sample. The information allows us to infer the uncertainty in sampling the original population.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_sampling.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_sampling.png 1x,/blog/image/no_skip_smm/bootstrap_sampling_2x.png 2x\"\r\nalt=\"Usually, we can infer the confidence interval of our interested statistic $ \\hat{\\theta} $ of our samples $ \\hat{X} $ using known formulae.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eUsually, we can infer the confidence interval of our interested statistic $ \\hat{\\theta} $ of our samples $ \\hat{X} $ using known formulae.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_sampling2.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_sampling2.png 1x,/blog/image/no_skip_smm/bootstrap_sampling2_2x.png 2x\"\r\nalt=\"When there are no known formulae, a simple way to study the uncertainty is to repeat the sampling process many times. It trades the reduction of uncertainty with the knowledge in uncertainty.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eWhen there are no known formulae, a simple way to study the uncertainty is to repeat the sampling process many times. It trades the reduction of uncertainty with the knowledge in uncertainty.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_resampling.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_resampling.png 1x,/blog/image/no_skip_smm/bootstrap_resampling_2x.png 2x\"\r\nalt=\"Resampling the only set of observed samples $ \\hat{X}^*_1 $ to infer the uncertainty in $ \\hat{\\theta_1} $ is much cheaper. We can resample $ \\hat{X_1} $ many more times than sampling $ X $. It allows us to have the knowledge in uncertainty without sacrificing a smaller uncertainty.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eResampling the only set of observed samples $ \\hat{X}^*_1 $ to infer the uncertainty in $ \\hat{\\theta_1} $ is much cheaper. We can resample $ \\hat{X_1} $ many more times than sampling $ X $. It allows us to have the knowledge in uncertainty without sacrificing a smaller uncertainty.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eFor simplicity, we will use a non-parameteric bootstrap to estimate some confidence intervals of the success rate. It will directly resample the observed samples. There are many popular non-parameteric bootstraps to choose from, basic (aka empirical/reverse percentile), bootstrap-t (studentized basic), \u003ca href=\"https://en.wikipedia.org/wiki/Percentile\"\u003epercentile\u003c/a\u003e, \u003ca href=\"https://en.wikipedia.org/wiki/Bias_of_an_estimator\"\u003ebias\u003c/a\u003e-corrected (BC), and bias-corrected and accelerated (BCa). They have different assumptions, so they work in different situations\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e. The basic bootstrap is based on $ \\hat{\\theta} - \\theta $ and requires it to be \u003ca href=\"https://en.wikipedia.org/wiki/Pivotal_quantity\"\u003epivotal\u003c/a\u003e. A pivot is a quantity that is independent of any parameters. It is like a normalized value. The bootstrap-t uses $ \\frac{\\hat{\\theta} - \\theta}{\\hat{\\sigma}} $ instead so it allows variation in the variance. On the contrary, the percentile bootstrap directly works on $ \\hat{\\theta} $. It only requires existence of a monotonically increasing function $ g $ such that $ \\hat{w} = g(\\hat{\\theta}) - g(\\theta) $ is a normal distribution of 0 mean. The amazing and magical part is that you do not need to provide $ g $ at all. The BC bootstrap extends the percentile bootstrap by supporting bias in $ \\hat{w} $. Furthermore, the BCa bootstrap supports bias and \u003ca href=\"https://en.wikipedia.org/wiki/Skewness\"\u003eskewness\u003c/a\u003e in $ \\hat{w} $. If you are interested in the theory behind it, I would recommend you to read the section \u0026quot;BOOTSTRAP CONFIDENCE INTERVALS\u0026quot; in \u003cem\u003eBootstrap confidence intervals: when, which, what? A practical guide for medical statisticians\u003c/em\u003e by James Carpenter and John Bithell\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eTo make non-parametric bootstraps compatible with the way we calculate the transition probabilities, we will simply throw away samples of levels starting with 97 to 99 lives this time. The levels limit the maximum life change. This condition is called \u003ca href=\"https://en.wikipedia.org/wiki/Censoring_(statistics)\"\u003eright censoring\u003c/a\u003e. For example, consider a level that starts with 97 lives and ends with 99 lives. The life change is +2. When applying the life change to a level that starts with 1 life, we do not know if it should end with 3 or 4 lives. A more advanced way to calculate the transition probabilities is to incorporate all levels into a distribution, but it requires the usage of parametric bootstraps. I will leave it for a future article.\u003c/p\u003e\n\u003cp\u003eIt is unclear if any one of the bootstraps above will work. We will conduct a simluation study of the coverages of the confidence intervals along the way. Using the same resampling technique, we can empirically test the coverages of the confidence intervals. The sample becomes a population, and this time we resample it to simulate the original sampling process. For each resample, we use the bootstraps to calculate the confidence intervals. Because we know the population, and so does the true success rate, we can count the number of instances in which the confidence intervals cover the true success rate. As a result, we can compare the cover rate with the desired rate to determine if the bootstraps work.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_ci.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_ci.png 1x,/blog/image/no_skip_smm/bootstrap_ci_2x.png 2x\"\r\nalt=\"An illustration of the procedure of the study of the coverages. The sample acts as a population. It is resampled to mimic the original sampling process. A confidence interval is calculated using the bootstrap for each set of resamples. Counting the number of confidence intervals that cover the statistic of the sample gives the actual coverage. It should be within random variations of the nominal coverage.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eAn illustration of the procedure of the study of the coverages. The sample acts as a population. It is resampled to mimic the original sampling process. A confidence interval is calculated using the bootstrap for each set of resamples. Counting the number of confidence intervals that cover the statistic of the sample gives the actual coverage. It should be within random variations of the nominal coverage.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003ch1 id=\"data-collection\"\u003eData Collection\u003c/h1\u003e\n\u003cp\u003eWe will study the winning probability of one of the most skilled players in Super Mario Marker 2, PangaeaPanga (aka Panga). His playthrough videos are available on his YouTube \u003ca href=\"https://www.youtube.com/watch?v=2L479zAldzI\u0026amp;list=PL8ooUSoeobwrpfeqSEsRgz0R1PKSjOiJV\"\u003eplaylist\u003c/a\u003e will provide us the data. First, we will need to gather the numbers of different life changes. Instead of tediously recording every event manually, we will use computer vision to automate it. We will identify frames containing the number of lives after the completion of each level, recognize the texts, and save them into a list.\u003c/p\u003e\n\u003ch2 id=\"frame-detection\"\u003eFrame Detection\u003c/h2\u003e\n\u003cp\u003eWe start by analyzing the information in the videos. Conveniently, Panga put his number of completed levels on the top right corner. He also showed all the level title scenes, which contained the number of lives. A simple handcrafted detector is more than enough for the job.\u003c/p\u003e\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/title_scene.png\"\r\nsrcset=\"/blog/ %!s(\u003cnil\u003e)\"\r\nalt=\"An example of the title scene. The blue and red bounding boxes show the features and the information to extract respectively.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eAn example of the title scene. The blue and red bounding boxes show the features and the information to extract respectively.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eThe title scenes are easily recognizable by a huge yellow box at the top and a black background. We pick some coordinates for the bounding boxes to count the pixels matching our targeted colors. The average colors will not work as our thresholds because the videos are encoded by lossy compression. The colors are distorted and differ at different locations and times. There are no other kinds of frames similar to this, so we can use some generous thresholds without worrying about much false detection of frames. We should be careful that not all the frames are as clean as the example above. Sometimes, there are emotes (icons) flying around and text overlays. We should use some smaller thresholds for the number of pixels.\u003c/p\u003e\n\u003cp\u003eUsing the first detected frame of each title scene to read the number of lives will give a poor result. The text showing the number of lives has a popup animation that enlarges from the bottom left. The text is too small at the beginning of the animation. To work around it, we wait for a larger text by counting the number of white pixels within the bounding box enclosing the widest left digit, 8. The threshold should be small enough to accommodate the thinnest digit, 1.\u003c/p\u003e\n\u003cp\u003eWe will refine the thresholds by examining the failure cases by running the program a couple of times after implementing the text recognition in the next step. Below is a peek of the final thresholds:\u003c/p\u003e\n\u003cstyle\u003e\r\n.color_block_shadow {\r\n    text-shadow:-0.5px 0.5px 2px #00000080;\r\n}\r\n\u003c/style\u003e\r\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFeature\u003c/th\u003e\n\u003cth\u003eColor\u003c/th\u003e\n\u003cth\u003eLower Threshold\u003c/th\u003e\n\u003cth\u003eUpper Threshold\u003c/th\u003e\n\u003cth\u003eArea\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eTittle banner\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#FBCB03\"\u003e■\u003c/span\u003e #FBCB03\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#E6BE00\"\u003e■\u003c/span\u003e #E6BE00\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#FFDC14\"\u003e■\u003c/span\u003e #FFDC14\u003c/td\u003e\n\u003ctd\u003e\u0026gt;70%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBackground\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#000000\"\u003e■\u003c/span\u003e #000000\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#000000\"\u003e■\u003c/span\u003e #000000\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#060606\"\u003e■\u003c/span\u003e #060606\u003c/td\u003e\n\u003ctd\u003e\u0026gt;70%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eStable life text\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#FFFFFF; text-shadow:-0.5px 0.5px 3px #00000080\"\u003e■\u003c/span\u003e #FFFFFF\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#C8C8C8;\"\u003e■\u003c/span\u003e #C8C8C8\u003c/td\u003e\n\u003ctd\u003e\u003cspan class=\"color_block_shadow\" style=\"color:#FFFFFF;\"\u003e■\u003c/span\u003e #FFFFFF\u003c/td\u003e\n\u003ctd\u003e\u0026gt;14%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe source code is available in the \u003ca href=\"https://github.com/keithyipkw/blog/tree/master/post_supplements%5Cno_skip_smm%22\"\u003eGitHub repository\u003c/a\u003e of this site. I will not show it and explain the implementation here. There are plenty of tutorials on fundamental computer vision on the Internet.\u003c/p\u003e\n\u003ch2 id=\"optical-character-recognition\"\u003eOptical Character Recognition\u003c/h2\u003e\n\u003cp\u003eAfter locating the frames, the next step is to recognize the text corresponding to the number of lives and completed levels. There are many ways to do it. Simple template matching provided by OpenCV should work, but we will use \u003ca href=\"https://github.com/tesseract-ocr/tesseract\"\u003eTesseract\u003c/a\u003e instead. Tesseract comes with pre-trained models, so we do not need to prepare any templates to match or train. Unfortunately, whitelisting the built-in pre-trained English model with digits just happens to perform poorly. It is probably because the architecture of the neural network does not handle whitelists well. Before trying to train a specialized model using the numbers from the frames or switching to template matching, we should try other pre-trained models first. A Tesseract contributor, Shreeshrii, shared their refined models in his \u003ca href=\"https://github.com/Shreeshrii/tessdata_shreetest\"\u003eGitHub repository\u003c/a\u003e. \u003ccode\u003edigits.traineddata\u003c/code\u003e works much better than the built-in English model.\u003c/p\u003e\n\u003ch2 id=\"data-cleansing\"\u003eData Cleansing\u003c/h2\u003e\n\u003cp\u003eAfter implementing the process, we get the level information that requires some data cleansing. We should not blindly accept the output as there may be errors in the output. Panga gave up several times which he had rough starts. Besides, there may be recognition errors. An easy way to spot major errors is to check for invalid level changes and life changes. Generally, each consecutive level should have +1 level change and less than or equal to +3 life change. For smaller incorrect life changes, the only way to eliminate them is to manually compare the output against the videos. We will skip it and assume that they are insignificant to the estimated success rate.\u003c/p\u003e\n\u003cp\u003eThere are only 12 erroneous levels out of 1058 levels. The error rate is 1.13%. At this point, it does not worth the time to improve the frame detector and text recognition. We should manually fix those errors.\u003c/p\u003e\n\u003ch1 id=\"simulation\"\u003eSimulation\u003c/h1\u003e\n\u003cp\u003eBefore proceeding to the calculation, we need to filter out levels starting with 97 to 99 lives. There are 726 remaining levels that we can use. He gained lives 52.5% of the time, lost lives 26.6% of the time, and neither 20.9% of the time. On average, he gained 0.233 lives after playing a level. The distribution is as follow:\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/pangas_life_changes.png\"\r\nsrcset=\"/blog/image/no_skip_smm/pangas_life_changes.png 1x,/blog/image/no_skip_smm/pangas_life_changes_2x.png 2x\"\r\nalt=\"Panga\u0026#39;s performance on the 726 levels.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003ePanga's performance on the 726 levels.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"sample-success-rate\"\u003eSample Success Rate\u003c/h2\u003e\n\u003cp\u003eNow we can calculate Panga's sample success rate of beating the challenge by applying the theory of Markov process. The calculation is also a building block in the bootstrap for calculating the confidence interval. The sample probability is 61.6%.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/pangas_sample_success_rates.png\"\r\nsrcset=\"/blog/image/no_skip_smm/pangas_sample_success_rates.png 1x,/blog/image/no_skip_smm/pangas_sample_success_rates_2x.png 2x\"\r\nalt=\"The estimated probability of Panga beating various numbers of levels in the endless challenge without skipping.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe estimated probability of Panga beating various numbers of levels in the endless challenge without skipping.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"bootstrap\"\u003eBootstrap\u003c/h2\u003e\n\u003cp\u003eWe will use the bootstrap module in \u003ca href=\"https://github.com/bashtage/arch\"\u003eArch\u003c/a\u003e to calculate the confidence interval. I choose it instead of SciPy because it is easier to experiment with. It provides more bootstrap methods and natively supports resuing the results when changing bootstrap methods.\u003c/p\u003e\n\u003ch3 id=\"simulation-study-of-coverages\"\u003eSimulation Study of Coverages\u003c/h3\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_ci.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_ci.png 1x,/blog/image/no_skip_smm/bootstrap_ci_2x.png 2x\"\r\nalt=\"Recall of the procedure of testing the coverage.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eRecall of the procedure of testing the coverage.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eIf you use Arch directly, you will encounter the following exception\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEmpirical probability used in bias correction is 0 or 1, and so bias cannot be corrected. This may occur in extremum statistics that are not well approximated by a normal in a finite sample.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor us, it happens when a simulated sample $ \\hat{X}_r^* $ excludes all occurrences of -15 lives and below. The maximum life loss is 14, so the survival probability of the first level is 100% for all the bootstrap resamples $ \\hat{X}_{r,s}^* $. Patching Arch to handle the problem is fairly easy and fast.\u003c/p\u003e\n\u003cp\u003eWe start by preliminarily checking the coverages of 95% confidence interval for several common bootstrap methods, basic, percentile, bias-corrected (BC), and bias-corrected and accelerated (BCa). 1000 bootstrap resamples repeating 1000 times is enough. It means that there are 1000 simulated samples $ \\hat{X}_r^* $ and 1000 bootstrap resamples $ \\hat{X}_{r,s}^* $ for each simulated samples. After obtaining a confidence interval for each of the 1000 true success rate $ \\hat{\\theta} $, we count numbers of confidence intervals that contain their corresponding success rate. I ran the test for levels from 1 to 3000 and plotted the graph below.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_ci_all_0.95.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_ci_all_0.95.png 1x,/blog/image/no_skip_smm/bootstrap_ci_all_0.95_2x.png 2x\"\r\nalt=\"The coverages of the 95% confidence intervals for 1000 bootstrap resamples repeating 1000 times.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe coverages of the 95% confidence intervals for 1000 bootstrap resamples repeating 1000 times.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cp\u003eThe dotted lines in the graph show the thresholds rejecting the null hypotheses that the corresponding measured coverages were the same as the nominal coverage of 95%. It was clear that the basic bootstrap gave different coverages. On the contrary, we could not reject the null hypotheses for the percentile, the BC, and the BCa bootstrap. You should be cautious that it did not mean that the coverages were the same as the nominal coverage. It is impossible to prove these null hypotheses. To tell if those methods gave the \u0026quot;same\u0026quot; coverages as the nominal coverage, we should perform \u003ca href=\"https://en.wikipedia.org/wiki/Equivalence_test\"\u003eequivalence tests\u003c/a\u003e. From now on, we will exclude the basic bootstrap and increase the number of resamples and repeats.\u003c/p\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_ci_0.95.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_ci_0.95.png 1x,/blog/image/no_skip_smm/bootstrap_ci_0.95_2x.png 2x\"\r\nalt=\"The coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cfigure\u003e\r\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLevels\u003c/th\u003e\n\u003cth\u003ePercentile\u003c/th\u003e\n\u003cth\u003eBC\u003c/th\u003e\n\u003cth\u003eBCa\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1000\u003c/td\u003e\n\u003ctd\u003e94.9%\u003c/td\u003e\n\u003ctd\u003e94.9%\u003c/td\u003e\n\u003ctd\u003e95.0%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2000\u003c/td\u003e\n\u003ctd\u003e94.8%\u003c/td\u003e\n\u003ctd\u003e94.8%\u003c/td\u003e\n\u003ctd\u003e95.2%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3000\u003c/td\u003e\n\u003ctd\u003e94.8%\u003c/td\u003e\n\u003ctd\u003e94.9%\u003c/td\u003e\n\u003ctd\u003e95.1%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\r\n\u003cfigcaption\u003eThe coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/bootstrap_ci_0.99.png\"\r\nsrcset=\"/blog/image/no_skip_smm/bootstrap_ci_0.99.png 1x,/blog/image/no_skip_smm/bootstrap_ci_0.99_2x.png 2x\"\r\nalt=\"The coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eThe coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003cfigure\u003e\r\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLevels\u003c/th\u003e\n\u003cth\u003ePercentile\u003c/th\u003e\n\u003cth\u003eBC\u003c/th\u003e\n\u003cth\u003eBCa\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1000\u003c/td\u003e\n\u003ctd\u003e99.0%\u003c/td\u003e\n\u003ctd\u003e99.0%\u003c/td\u003e\n\u003ctd\u003e99.2%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2000\u003c/td\u003e\n\u003ctd\u003e99.0%\u003c/td\u003e\n\u003ctd\u003e99.0%\u003c/td\u003e\n\u003ctd\u003e99.2%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3000\u003c/td\u003e\n\u003ctd\u003e98.9%\u003c/td\u003e\n\u003ctd\u003e99.0%\u003c/td\u003e\n\u003ctd\u003e99.2%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\r\n\u003cfigcaption\u003eThe coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\u003cp\u003eThey all performed similarly. I will not perform the equivalence tests here. Let us assume that they are equivalent to our use. We will use the BCa for the analysis.\u003c/p\u003e\n\u003ch1 id=\"result\"\u003eResult\u003c/h1\u003e\n\u003cp\u003eCombining the sample probability with the bootstrap result using the BCa for 40000 resamples gave the following result. As a bonus, I calculated all the probabilities for levels up to 3000. You should keep in mind that the probabilities of the confidence intervals shown below refer to the long-run frequencies of covering the true success rates for (hypothetical) repeated random sampling. A particular confidence interval either covers or does not cover the true success rate. After getting a sample and revealing the confidence interval, the cover rate of the confidence interval is only epistemic (one's ignorance). Further interpretations require a deeper understanding of the properties of bootstrapping. The definition of confidence intervals is loose in which it only concerns the cover rate. Some procedures give opposite widths or counter-intuitive intervals. Richard D. Morey, Rink Hoekstra, Jeffrey N. Rouder, Michael D. Lee, and Eric-Jan Wagenmakers explained the issues nicely in \u003cem\u003eThe fallacy of placing confidence in confidence intervals\u003c/em\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e. Below is the result:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLevels\u003c/th\u003e\n\u003cth\u003eProbability\u003c/th\u003e\n\u003cth\u003e95% CI\u003c/th\u003e\n\u003cth\u003e99% CI\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1000\u003c/td\u003e\n\u003ctd\u003e61.6%\u003c/td\u003e\n\u003ctd\u003e[15.7%, 85.4%]\u003c/td\u003e\n\u003ctd\u003e[5.84%, 89.4%]\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2000\u003c/td\u003e\n\u003ctd\u003e58.9%\u003c/td\u003e\n\u003ctd\u003e[8.38%, 86%]\u003c/td\u003e\n\u003ctd\u003e[1.78%, 90.2%]\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3000\u003c/td\u003e\n\u003ctd\u003e56.3%\u003c/td\u003e\n\u003ctd\u003e[3.96%, 86%]\u003c/td\u003e\n\u003ctd\u003e[0.43%, 90.1%]\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/pangas_probabilities.png\"\r\nsrcset=\"/blog/image/no_skip_smm/pangas_probabilities.png 1x,/blog/image/no_skip_smm/pangas_probabilities_2x.png 2x\"\r\nalt=\"Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eProbabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/pangas_probabilities_simplified.png\"\r\nsrcset=\"/blog/image/no_skip_smm/pangas_probabilities_simplified.png 1x,/blog/image/no_skip_smm/pangas_probabilities_simplified_2x.png 2x\"\r\nalt=\"Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eProbabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/pangas_probability_1000.png\"\r\nsrcset=\"/blog/image/no_skip_smm/pangas_probability_1000.png 1x,/blog/image/no_skip_smm/pangas_probability_1000_2x.png 2x\"\r\nalt=\"Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eProbability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\r\n\r\n\u003cfigure\u003e\r\n\u003cimg\r\nclass=\"mx-auto leading-none\"\r\nsrc=\"/blog/image/no_skip_smm/pangas_cdf_1000.png\"\r\nsrcset=\"/blog/image/no_skip_smm/pangas_cdf_1000.png 1x,/blog/image/no_skip_smm/pangas_cdf_1000_2x.png 2x\"\r\nalt=\"Cumulative probability of Panga successfully beating 1000 levels.\"\u003e\r\n\u003cfigcaption class=\"text-center text-raven-500\"\u003e\r\n\u003cp\u003eCumulative probability of Panga successfully beating 1000 levels.\u003c/p\u003e\r\n\u003c/figcaption\u003e\r\n\u003c/figure\u003e\r\n\n\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eUndoubtedly, beating 1000 endless expert levels without skipping in Super Mario Marker 2 is hard for most players. It is not for ones without top-notch skills and strong determination. This article has quantified the difficulty by using the theory of the Markov process, bootstrapping, and one of the best players' data. You are likely to have even more questions about this challenge other than the success rate. In the future, we will delve deeper into the details in \u003ca href=\"https://keithyipkw.github.io/blog/no_skip_smm_part_2/\"\u003epart 2\u003c/a\u003e.\u003c/p\u003e\n\u003csection class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\" role=\"doc-endnote\"\u003e\n\u003cp\u003eA tweet by Nintendo of America on 4 Sep, 2020. \u003ca href=\"https://twitter.com/NintendoAmerica/status/1301928016004644864\"\u003ehttps://twitter.com/NintendoAmerica/status/1301928016004644864\u003c/a\u003e \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\" role=\"doc-endnote\"\u003e\n\u003cp\u003eCarpenter, J. and Bithell, J. (2000), Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians. Statist. Med., 19: 1141-1164. \u003ca href=\"https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9\"\u003ehttps://doi.org/10.1002/(SICI)1097-0258(20000515)19:9\u003c/a\u003e\u0026lt;1141::AID-SIM479\u0026gt;3.0.CO;2-F \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\" role=\"doc-endnote\"\u003e\n\u003cp\u003eMorey, R.D., Hoekstra, R., Rouder, J.N. et al. The fallacy of placing confidence in confidence intervals. Psychon Bull Rev 23, 103–123 (2016). \u003ca href=\"https://doi.org/10.3758/s13423-015-0947-8\"\u003ehttps://doi.org/10.3758/s13423-015-0947-8\u003c/a\u003e \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/section\u003e"}</script><link rel=preload as=script href="/blog/bundle.js?v=1661093204"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://stats.g.doubleclick.net><link rel=preconnect href=https://www.googleadservices.com><link rel=preload as=script href="https://www.googletagmanager.com/gtag/js?id=G-5Y5GJS2SYG"><script src="https://www.googletagmanager.com/gtag/js?id=G-5Y5GJS2SYG"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('consent','default',{'ad_storage':'denied','analytics_storage':'denied'});gtag('js',new Date());gtag('config','G-5Y5GJS2SYG');</script><script>MathJax={loader:{load:['[tex]/mathtools']},tex:{inlineMath:[['$','$']],packages:{'[+]':['mathtools']},tags:'ams'}};</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><header id=nav class=header><div class="ax-l-i max-w-7xl"><div class=ax-logo><a class=block href=/blog/ title=STEM><img src=/blog/image/brand/logo.png alt=STEM></a></div><div class=ax-user></div></div></header><main><div class=default-single><div class="ax-title ax-l-o"><div class="ax-l-i max-w-7xl"><h1 class="post-title font-content-title font-semibold leading-tight tracking-default text-40">How Difficult is 1000 Endless Expert Levels Without Skipping in Super Mario Marker 2?</h1><div class="ax-meta flex items-center mt-5"><div class="flex-grow min-w-0"><div class="flex items-center"><div class="flex-shrink-0 leading-tight font-content-sans"><div class="block text-sm">Keith Yip</div><time class="text-sm text-raven-500" datetime=2022-01-17T13:03:00Z>Jan 17, 2022 9:03PM</time></div></div></div></div></div></div><div class="flex flex-wrap justify-center"><a class="rounded font-content-sans font-semibold text-raven-700 bg-raven-100 hover:bg-raven-200 py-2 px-4 m-2" href=https://keithyipkw.github.io/blog/tags/statistics/>Statistics</a></div><div class="ax-content ax-l-o"><div class="ax-l-i max-w-7xl"><article class=cdata><p>Mario games are fun and challenging. Nintendo has been spending a great deal of effect to design and implement them. In 2015, they released Super Mario Maker. The community could finally legally make and share their levels. With their creativity and dedication, many fun levels which Nintendo would not make were born. Unsurprisingly, the community also made loads of trash levels. Some were only by-products of learning to be great level makers, but the others were purposely built to torment players. In 2019, Nintendo released a sequel to the game Super Mario Marker 2. There has been a new game mode, endless challenge.</p><h1 id=no-skip-endless-challenge>No-skip Endless Challenge</h1><p>In the endless challenge, you try to complete as many levels the game throws at you as possible. You pick a difficulty and have a certain number of lives to begin. For expert difficulty, you have 15 lives at the start.</p><table><thead><tr><th>Difficulty</th><th>Number of Starting Lives</th></tr></thead><tbody><tr><td>Easy</td><td>5</td></tr><tr><td>Normal</td><td>5</td></tr><tr><td>Expert</td><td>15</td></tr><tr><td>Super Expert</td><td>30</td></tr></tbody></table><p>Each time you die on a level, you lose a life. When you have no remaining lives, the run ends. There are multiple ways to gain lives, but you can only redeem them after clearing a level. If there is a flagpole at the end of a level and you touch the top, you receive a life. Similar to other Mario games, collecting a green mushroom or 100 coins grant you a life too. Besides, you can stomp on multiple enemies without touching the ground or using a shell to kill them consecutively to gain lives as well. You can only redeem as most 3 lives in a level because the game limit that to prevent you to farm lives. A generous level maker will unconditionally give you 3 lives or loads of coins. Conversely, an inexperienced or mean one will give you nothing and even block you from reaching the top of the flagpole to gain a life. In addition to the life gain limit in a level, you can have maximally 99 lives.</p><p>The game randomly chooses a level made by the community according to the difficulty as your next level. The exact formula of how the game determines the difficulty of a level is not publically known. It is however strongly correlated to the level clear rate ($ \frac{\text{#clears}}{\text{#trials}} $), player clear rate ($ \frac{\text{#winning players}}{\text{#trying players}} $), and the level maker's number of upload trials. For expert difficulty, you should expect to occasionally encounter a level with a clear rate as low as 2%. For such a level, the community wins 1 out of 50 trials. Very rarely, you will encounter a level with a difficulty like the super expert because the formula does not rate the level accurately enough. It seems to happen more often on new levels which have more uncertainty.</p><p>The game allows you to skip levels so you will have a more enjoyable experience in the endless challenge. Your run will not immediately end because you encounter a level poorly made, requiring some skills or knowledge you do not have. However, what hardcore gamers want is the exact opposite. Skipping makes the challenge too easy, so they handicap themselves by forbidding it.</p><p>Your life gain on each level needs to be at least on par with your loss on average to keep a run going in the long term. If you are an average player having only a clear rate of a few percent on an expert level, you virtually have no hope of passing 1000 levels, not even 10 levels. Besides a high clear rate, you also need a lower variance in your life change so you can resist a longer streak of bad levels.</p><h1 id=theory>Theory</h1><p>The most reliable and straightforward way to estimate the difficulty of the no-skip endless challenge is to use an average success rate of many runs. It is the same as estimating the fairness of a coin. We can use the <a href=https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval>binomial proportion</a> to obtain the uncertainty in terms of the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence interval</a>. A confidence interval is an interval that covers the true parameter with a certain amount of long-run frequency for (hypothetically) repeatedly random sampling. The sampling process works well for poorly performed players whose success rates are close to 0 because their runs do not last long. It is not the case for skilled players. A run may take many hours. For example, with an average of 3 minutes per level, a successful run will take 50 hours to complete. Without many runs, the uncertainty is large, so the overall confidence intervals are too. For example, the 99% confidence interval of a win and a loss is [6.2%, 93.8%]. Therefore, It is worth putting efforts to squeeze out more information from the existing data to shrink the uncertainty. We will model the problem as a Markov process and use the bootstrap to infer the success rate and the confidence intervals. Generally, the estimated success rate should be closer to the true value for all possible true values and samples. Similarly, the overall confidence intervals should be tighter.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/example_life_changes.png srcset="/blog/image/no_skip_smm/example_life_changes.png 1x,/blog/image/no_skip_smm/example_life_changes_2x.png 2x" alt="An example of life changes after beating each of the 1000 levels. The sample success rate is about 50%."><figcaption class="text-center text-raven-500"><p>An example of life changes after beating each of the 1000 levels. The sample success rate is about 50%.</p></figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/example_cdf_1.png srcset="/blog/image/no_skip_smm/example_cdf_1.png 1x,/blog/image/no_skip_smm/example_cdf_1_2x.png 2x" alt="The cumulative probability functions calculated by the two methods. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on the distribution of the life changes above. The other one calculated by the binomial proportion is based on one win and one loss."><figcaption class="text-center text-raven-500"><p>The cumulative probability functions calculated by the two methods. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on the distribution of the life changes above. The other one calculated by the binomial proportion is based on one win and one loss.</p></figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/example_cdf_2.png srcset="/blog/image/no_skip_smm/example_cdf_2.png 1x,/blog/image/no_skip_smm/example_cdf_2_2x.png 2x" alt="The cumulative probability functions calculated by the two methods with doubling the samples. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on doubling the distribution of the life changes above. The other one calculated by the binomial proportion is based on two wins and two losses."><figcaption class="text-center text-raven-500"><p>The cumulative probability functions calculated by the two methods with doubling the samples. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on doubling the distribution of the life changes above. The other one calculated by the binomial proportion is based on two wins and two losses.</p></figcaption></figure><p>We begin by identifying the essence of the process. At the start of a run, a player has 15 lives. The game randomly draws a level as the next level. After playing a level, the number of life may change with an amount ranging from losing all their lives to gaining 3 lives. If the player loses all their lives, the run ends. If the player loses all their lives, the run ends. Conversely, if they gain so many lives that the total exceeds 99, it will still be 99. The number of life and the change are sufficient to describe the player state and a level, respectively.</p><p>Because of the game being closed source, Nintendo not publishing any detail, and insufficient samples, we need to make a few assumptions and simplifications:</p><ol><li>The game draws a level independent of the number of lives.</li><li>The game draws a level independent of the completed levels.</li><li>The player's performance is independent of the previous levels.</li><li>The player performs practically the same when their number of lives is from 1 to 96 or 97 to 99.</li><li>The difficulty of the levels is approximately the same across different runs.</li></ol><p>There has been no information about the independence in the first assumption. Considering that it seems to be and tight budgets are common in the game industry, it is reasonable to assume that Nintendo chose the simplest implementation. The second assumption is at least a close approximation of the real mechanism. There are 20 million levels<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> as of September 2020. Even if a small subset has the expert difficulty, there should still be enough levels that drawing 1000 levels with replacements is approximately the same as drawing without replacements. Thus, we do not need to worry about which case it is. We can simply model it as drawing with replacements. The remaining assumptions are to reduce the sample requirements. Otherwise, we will need a more sophisticated model or much more samples. Careful analysis may disproof some of the assumptions, but it is out of the scope of this article. Regardless, the following will at least serve as an entry point for more sophisticated analysis even if you do not agree with the assumptions.</p><h2 id=modelling-as-a-markov-process>Modelling as a Markov Process</h2><p>An easy way of determining the probability is to simulate many runs. Such a simulation can run 500 times per second on my computer. However, because the variance of the runs is very high, the success rate needs 1 million runs to start converging. If all we need is the expected success rate, it is acceptable to let it run for 33 minutes. But we cannot ignore the potentially significant uncertainty in the collected statistics. We can simulate the sampling process and run the 1 million simulations many times, say 1,000, to approximate the uncertainty. That alone will take 23 days of computational time. To ensure that 1,000 simulations are enough, we should repeat it many times too, say 1,000 times, to verify it. The approach quickly becomes impractical if we want to be thorough. Instead, if we model the problem as a Markov process, we can calculate the success rates analytically 2,500 times per second, not once per 33 minutes. Since the solutions are exact, we eliminate the uncertainty in the simulation.</p><p>A Markov process is a random process in which the current state determines the future state. In our case, the player's number of lives is the state. By knowing the probability of different life changes after completing a level, we can predict the probabilities of the future number of lives. For example, suppose that a player currently has 10 lives. The probabilities of -1 life, +0 life, and +1 life after finishing the next level are 20%, 30%, and 50%, respectively. The probabilities of having 9, 10, and 11 lives after finishing the next level are 20%, 30%, and 50%, respectively.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/state_change_individual.png srcset="/blog/image/no_skip_smm/state_change_individual.png 1x,/blog/image/no_skip_smm/state_change_individual_2x.png 2x" alt="The probabilities of the future state can be calculated by considering each mututally exclusive branch separately."><figcaption class="text-center text-raven-500"><p>The probabilities of the future state can be calculated by considering each mututally exclusive branch separately.</p></figcaption></figure><p>Furthermore, different numbers of lives are mutually exclusive states, and so do life changes as mutually exclusive events. By the law of total probability, we can separately consider different states then sum the probabilities for all the outcomes. Referring to the example above but now the probabilities of the current state having 9 and 10 lives are 40% and 60%, respectively, the probability of having 9 lives after finishing the next level is</p>$$
\begin{aligned}
P_\text{next}(\textcolor{red}{♥}=9) ={} & P_\text{current}(\textcolor{red}{♥}=9) \times P(\Delta\textcolor{red}{♥}=0) \\
& + P_\text{current}(\textcolor{red}{♥}=10) \times P(\Delta\textcolor{red}{♥}=-1) \\
={} & 40\% \times 30\% + 60\% \times 20\% \\
={} & 24\%
\end{aligned}
$$
<p>Similarly, the probabilities for 8, 10, and 11 lives are 8%, 38%, and 30%, respectively.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/state_change_combined.png srcset="/blog/image/no_skip_smm/state_change_combined.png 1x,/blog/image/no_skip_smm/state_change_combined_2x.png 2x" alt="The probability of a future state can be calculated separately then summed."><figcaption class="text-center text-raven-500"><p>The probability of a future state can be calculated separately then summed.</p></figcaption></figure><p>It is more convenient to organize the calculation in the following matrix form. In addition, the matrix form allows us to leverage accelerated processings by computers during bootstrapping.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/state_change_matrix.png srcset="/blog/image/no_skip_smm/state_change_matrix.png 1x,/blog/image/no_skip_smm/state_change_matrix_2x.png 2x" alt="The calculation in the matrix form."><figcaption class="text-center text-raven-500"><p>The calculation in the matrix form.</p></figcaption></figure><p>For the actual problem, let</p><ul><li>$ s_k $ be a 100-dimensional row vector for the states after finishing $ k $ levels,</li><li>$ P_s(k,\textcolor{red}{♥}=n) $ be the probability of the state after finishing $ k \in \set{0,1,2,\ldots} $ levels and having $ n \in \set{0,1,2,\ldots,99} $ lives,</li><li>$ T $ be a 100×100 right stochastic matrix,</li><li>$ P_t(\textcolor{red}{♥}=n,\Delta\textcolor{red}{♥}=m) $ be the probability of transiting from having $ n $ lives to $ (n+m) \in \set{0,1,2,\ldots,99} $ lives.</li></ul>$$
\begin{aligned}
s_k &= \begin{bmatrix}
P_s(k,\textcolor{red}{♥}=0) & P_s(k,\textcolor{red}{♥}=1) & \ldots & P_s(k,\textcolor{red}{♥}=99)
\end{bmatrix} \\ \\
T &= \begin{bmatrix}
1 & 0 & \ldots & 0 \\
P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=-1) & P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=0) & \ldots & P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=98) \\
\vdots & \vdots & \ddots & \vdots \\
P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=-99) & P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=-98) & \ldots & P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=0) \\
\end{bmatrix} \\ \\
s_{k+1} &= s_kT \\

\end{aligned}
$$
<p>The first row in $ T $ is a twist. It allows us to continue to apply the transition to the state even after a run end. This row represents how the <a href=https://en.wikipedia.org/wiki/Absorbing_Markov_chain>absorbing state</a> $ \textcolor{red}{♥}=0 $ is stuck there.</p><p>The other rows in $ T $ are mostly some offset versions of the general life change probabilities. Generally, they are the same as the corresponding transition probabilities for each state, i.e., $ P_t(\textcolor{red}{♥}=n,\Delta\textcolor{red}{♥}=m)=P(\Delta\textcolor{red}{♥}=m) $ where $ P(\Delta\textcolor{red}{♥}=m) $ is the probability of life change of $ m \in \set{0,1,2,\ldots,99} $ without considering the minimum (0) and maximum (99) numbers of lives. However, the transition probabilities for 0 lives ought to include the life change probabilities causing the number to drop below 0 and vice versa for 99 lives.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/state_change_transition_row.png srcset="/blog/image/no_skip_smm/state_change_transition_row.png 1x,/blog/image/no_skip_smm/state_change_transition_row_2x.png 2x" alt="In this example, the life change probabilities are for ranging from -3 lives to +1 life. The transition probabilties for the state having 1 life to the state having 0 lives are the sum of the -3, -2 and -1 life probability."><figcaption class="text-center text-raven-500"><p>In this example, the life change probabilities are for ranging from -3 lives to +1 life. The transition probabilties for the state having 1 life to the state having 0 lives are the sum of the -3, -2 and -1 life probability.</p></figcaption></figure><p>Formally, the transition probabilities are</p>$$
\begin{aligned}
P_t(\textcolor{red}{♥}=n,\Delta\textcolor{red}{♥}=m) = \begin{dcases}
\sum_{q=-\infty}^{m} P(\Delta\textcolor{red}{♥}=q),& \text{if } n+m=0\\
\sum_{q=m}^{\infty} P(\Delta\textcolor{red}{♥}=q),& \text{if } n+m=99\\
P(\Delta\textcolor{red}{♥}=m), & \text{otherwise}
\end{dcases}

\end{aligned}
$$<p>Starting from the initial state that is 100%, by applying the state change repeatedly, we have the probabilities of the state after finishing any number of levels.</p>$$
\begin{aligned}
s_k &= s_0T^k
\end{aligned}
$$
<p>We can then calculate the probability of clearing a certain number of levels, and so that of beating the challenge. The probability of beating $ k $ levels is</p>$$
\begin{aligned}
P(k) &= 1 - P_s(k,\textcolor{red}{♥}=0) \\
&= \sum_{\textcolor{red}{♥}=1}^{99} P_s(k,\textcolor{red}{♥})
\end{aligned}
$$<h2 id=uncertainty-estimation-by-bootstrapping>Uncertainty Estimation by Bootstrapping</h2><p>Sampling the frequencies of different life changes allows us to infer the <a href=https://en.wikipedia.org/wiki/Statistical_population>population</a>, the true probabilities of life changes, and then the parameter, the success rate. It is impossible to know the exact values because it requires the player to play every level infinitely many times to measure the frequencies of different life changes. When we only observe a finite number of samples, there is a random variation between the sample and the population. It is crucial to know the possible variations. Otherwise, we do not know how much the estimation is off. In this article, we will go with the <a href=https://en.wikipedia.org/wiki/Frequentist_inference>frequentist inference</a>. For some simple and common problems, there are formulae to calculate the variations in terms of confidence intervals. However, there seems to be none for this problem. We will use a simulation approach instead.</p><p>Bootstrapping is a resampling method based on <a href=https://en.wikipedia.org/wiki/Monte_Carlo_method>Monte Carlo</a> simulation. The core idea is very simple but yet powerful. We mimic the sampling process of the population by resampling the sample. The sample becomes a known population. With many resamples, our desired <a href=https://en.wikipedia.org/wiki/Statistic>statistic</a>, the sample success rate, forms a distribution that we can compare with the sample. The information allows us to infer the uncertainty in sampling the original population.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_sampling.png srcset="/blog/image/no_skip_smm/bootstrap_sampling.png 1x,/blog/image/no_skip_smm/bootstrap_sampling_2x.png 2x" alt="Usually, we can infer the confidence interval of our interested statistic $ \hat{\theta} $ of our samples $ \hat{X} $ using known formulae."><figcaption class="text-center text-raven-500"><p>Usually, we can infer the confidence interval of our interested statistic $ \hat{\theta} $ of our samples $ \hat{X} $ using known formulae.</p></figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_sampling2.png srcset="/blog/image/no_skip_smm/bootstrap_sampling2.png 1x,/blog/image/no_skip_smm/bootstrap_sampling2_2x.png 2x" alt="When there are no known formulae, a simple way to study the uncertainty is to repeat the sampling process many times. It trades the reduction of uncertainty with the knowledge in uncertainty."><figcaption class="text-center text-raven-500"><p>When there are no known formulae, a simple way to study the uncertainty is to repeat the sampling process many times. It trades the reduction of uncertainty with the knowledge in uncertainty.</p></figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_resampling.png srcset="/blog/image/no_skip_smm/bootstrap_resampling.png 1x,/blog/image/no_skip_smm/bootstrap_resampling_2x.png 2x" alt="Resampling the only set of observed samples $ \hat{X}^*_1 $ to infer the uncertainty in $ \hat{\theta_1} $ is much cheaper. We can resample $ \hat{X_1} $ many more times than sampling $ X $. It allows us to have the knowledge in uncertainty without sacrificing a smaller uncertainty."><figcaption class="text-center text-raven-500"><p>Resampling the only set of observed samples $ \hat{X}^*_1 $ to infer the uncertainty in $ \hat{\theta_1} $ is much cheaper. We can resample $ \hat{X_1} $ many more times than sampling $ X $. It allows us to have the knowledge in uncertainty without sacrificing a smaller uncertainty.</p></figcaption></figure><p>For simplicity, we will use a non-parameteric bootstrap to estimate some confidence intervals of the success rate. It will directly resample the observed samples. There are many popular non-parameteric bootstraps to choose from, basic (aka empirical/reverse percentile), bootstrap-t (studentized basic), <a href=https://en.wikipedia.org/wiki/Percentile>percentile</a>, <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>bias</a>-corrected (BC), and bias-corrected and accelerated (BCa). They have different assumptions, so they work in different situations<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. The basic bootstrap is based on $ \hat{\theta} - \theta $ and requires it to be <a href=https://en.wikipedia.org/wiki/Pivotal_quantity>pivotal</a>. A pivot is a quantity that is independent of any parameters. It is like a normalized value. The bootstrap-t uses $ \frac{\hat{\theta} - \theta}{\hat{\sigma}} $ instead so it allows variation in the variance. On the contrary, the percentile bootstrap directly works on $ \hat{\theta} $. It only requires existence of a monotonically increasing function $ g $ such that $ \hat{w} = g(\hat{\theta}) - g(\theta) $ is a normal distribution of 0 mean. The amazing and magical part is that you do not need to provide $ g $ at all. The BC bootstrap extends the percentile bootstrap by supporting bias in $ \hat{w} $. Furthermore, the BCa bootstrap supports bias and <a href=https://en.wikipedia.org/wiki/Skewness>skewness</a> in $ \hat{w} $. If you are interested in the theory behind it, I would recommend you to read the section "BOOTSTRAP CONFIDENCE INTERVALS" in <em>Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians</em> by James Carpenter and John Bithell<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>To make non-parametric bootstraps compatible with the way we calculate the transition probabilities, we will simply throw away samples of levels starting with 97 to 99 lives this time. The levels limit the maximum life change. This condition is called <a href=https://en.wikipedia.org/wiki/Censoring_(statistics)>right censoring</a>. For example, consider a level that starts with 97 lives and ends with 99 lives. The life change is +2. When applying the life change to a level that starts with 1 life, we do not know if it should end with 3 or 4 lives. A more advanced way to calculate the transition probabilities is to incorporate all levels into a distribution, but it requires the usage of parametric bootstraps. I will leave it for a future article.</p><p>It is unclear if any one of the bootstraps above will work. We will conduct a simluation study of the coverages of the confidence intervals along the way. Using the same resampling technique, we can empirically test the coverages of the confidence intervals. The sample becomes a population, and this time we resample it to simulate the original sampling process. For each resample, we use the bootstraps to calculate the confidence intervals. Because we know the population, and so does the true success rate, we can count the number of instances in which the confidence intervals cover the true success rate. As a result, we can compare the cover rate with the desired rate to determine if the bootstraps work.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_ci.png srcset="/blog/image/no_skip_smm/bootstrap_ci.png 1x,/blog/image/no_skip_smm/bootstrap_ci_2x.png 2x" alt="An illustration of the procedure of the study of the coverages. The sample acts as a population. It is resampled to mimic the original sampling process. A confidence interval is calculated using the bootstrap for each set of resamples. Counting the number of confidence intervals that cover the statistic of the sample gives the actual coverage. It should be within random variations of the nominal coverage."><figcaption class="text-center text-raven-500"><p>An illustration of the procedure of the study of the coverages. The sample acts as a population. It is resampled to mimic the original sampling process. A confidence interval is calculated using the bootstrap for each set of resamples. Counting the number of confidence intervals that cover the statistic of the sample gives the actual coverage. It should be within random variations of the nominal coverage.</p></figcaption></figure><h1 id=data-collection>Data Collection</h1><p>We will study the winning probability of one of the most skilled players in Super Mario Marker 2, PangaeaPanga (aka Panga). His playthrough videos are available on his YouTube <a href="https://www.youtube.com/watch?v=2L479zAldzI&list=PL8ooUSoeobwrpfeqSEsRgz0R1PKSjOiJV">playlist</a> will provide us the data. First, we will need to gather the numbers of different life changes. Instead of tediously recording every event manually, we will use computer vision to automate it. We will identify frames containing the number of lives after the completion of each level, recognize the texts, and save them into a list.</p><h2 id=frame-detection>Frame Detection</h2><p>We start by analyzing the information in the videos. Conveniently, Panga put his number of completed levels on the top right corner. He also showed all the level title scenes, which contained the number of lives. A simple handcrafted detector is more than enough for the job.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/title_scene.png srcset="/blog/ %!s(<nil>)" alt="An example of the title scene. The blue and red bounding boxes show the features and the information to extract respectively."><figcaption class="text-center text-raven-500"><p>An example of the title scene. The blue and red bounding boxes show the features and the information to extract respectively.</p></figcaption></figure><p>The title scenes are easily recognizable by a huge yellow box at the top and a black background. We pick some coordinates for the bounding boxes to count the pixels matching our targeted colors. The average colors will not work as our thresholds because the videos are encoded by lossy compression. The colors are distorted and differ at different locations and times. There are no other kinds of frames similar to this, so we can use some generous thresholds without worrying about much false detection of frames. We should be careful that not all the frames are as clean as the example above. Sometimes, there are emotes (icons) flying around and text overlays. We should use some smaller thresholds for the number of pixels.</p><p>Using the first detected frame of each title scene to read the number of lives will give a poor result. The text showing the number of lives has a popup animation that enlarges from the bottom left. The text is too small at the beginning of the animation. To work around it, we wait for a larger text by counting the number of white pixels within the bounding box enclosing the widest left digit, 8. The threshold should be small enough to accommodate the thinnest digit, 1.</p><p>We will refine the thresholds by examining the failure cases by running the program a couple of times after implementing the text recognition in the next step. Below is a peek of the final thresholds:</p><style>.color_block_shadow{text-shadow:-.5px .5px 2px #00000080}</style><table><thead><tr><th>Feature</th><th>Color</th><th>Lower Threshold</th><th>Upper Threshold</th><th>Area</th></tr></thead><tbody><tr><td>Tittle banner</td><td><span class=color_block_shadow style=color:#fbcb03>■</span> #FBCB03</td><td><span class=color_block_shadow style=color:#e6be00>■</span> #E6BE00</td><td><span class=color_block_shadow style=color:#ffdc14>■</span> #FFDC14</td><td>>70%</td></tr><tr><td>Background</td><td><span class=color_block_shadow style=color:#000>■</span> #000000</td><td><span class=color_block_shadow style=color:#000>■</span> #000000</td><td><span class=color_block_shadow style=color:#060606>■</span> #060606</td><td>>70%</td></tr><tr><td>Stable life text</td><td><span class=color_block_shadow style="color:#fff;text-shadow:-.5px .5px 3px #00000080">■</span> #FFFFFF</td><td><span class=color_block_shadow style=color:#c8c8c8>■</span> #C8C8C8</td><td><span class=color_block_shadow style=color:#fff>■</span> #FFFFFF</td><td>>14%</td></tr></tbody></table><p>The source code is available in the <a href=https://github.com/keithyipkw/blog/tree/master/post_supplements%5Cno_skip_smm%22>GitHub repository</a> of this site. I will not show it and explain the implementation here. There are plenty of tutorials on fundamental computer vision on the Internet.</p><h2 id=optical-character-recognition>Optical Character Recognition</h2><p>After locating the frames, the next step is to recognize the text corresponding to the number of lives and completed levels. There are many ways to do it. Simple template matching provided by OpenCV should work, but we will use <a href=https://github.com/tesseract-ocr/tesseract>Tesseract</a> instead. Tesseract comes with pre-trained models, so we do not need to prepare any templates to match or train. Unfortunately, whitelisting the built-in pre-trained English model with digits just happens to perform poorly. It is probably because the architecture of the neural network does not handle whitelists well. Before trying to train a specialized model using the numbers from the frames or switching to template matching, we should try other pre-trained models first. A Tesseract contributor, Shreeshrii, shared their refined models in his <a href=https://github.com/Shreeshrii/tessdata_shreetest>GitHub repository</a>. <code>digits.traineddata</code> works much better than the built-in English model.</p><h2 id=data-cleansing>Data Cleansing</h2><p>After implementing the process, we get the level information that requires some data cleansing. We should not blindly accept the output as there may be errors in the output. Panga gave up several times which he had rough starts. Besides, there may be recognition errors. An easy way to spot major errors is to check for invalid level changes and life changes. Generally, each consecutive level should have +1 level change and less than or equal to +3 life change. For smaller incorrect life changes, the only way to eliminate them is to manually compare the output against the videos. We will skip it and assume that they are insignificant to the estimated success rate.</p><p>There are only 12 erroneous levels out of 1058 levels. The error rate is 1.13%. At this point, it does not worth the time to improve the frame detector and text recognition. We should manually fix those errors.</p><h1 id=simulation>Simulation</h1><p>Before proceeding to the calculation, we need to filter out levels starting with 97 to 99 lives. There are 726 remaining levels that we can use. He gained lives 52.5% of the time, lost lives 26.6% of the time, and neither 20.9% of the time. On average, he gained 0.233 lives after playing a level. The distribution is as follow:</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/pangas_life_changes.png srcset="/blog/image/no_skip_smm/pangas_life_changes.png 1x,/blog/image/no_skip_smm/pangas_life_changes_2x.png 2x" alt="Panga's performance on the 726 levels."><figcaption class="text-center text-raven-500"><p>Panga's performance on the 726 levels.</p></figcaption></figure><h2 id=sample-success-rate>Sample Success Rate</h2><p>Now we can calculate Panga's sample success rate of beating the challenge by applying the theory of Markov process. The calculation is also a building block in the bootstrap for calculating the confidence interval. The sample probability is 61.6%.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/pangas_sample_success_rates.png srcset="/blog/image/no_skip_smm/pangas_sample_success_rates.png 1x,/blog/image/no_skip_smm/pangas_sample_success_rates_2x.png 2x" alt="The estimated probability of Panga beating various numbers of levels in the endless challenge without skipping."><figcaption class="text-center text-raven-500"><p>The estimated probability of Panga beating various numbers of levels in the endless challenge without skipping.</p></figcaption></figure><h2 id=bootstrap>Bootstrap</h2><p>We will use the bootstrap module in <a href=https://github.com/bashtage/arch>Arch</a> to calculate the confidence interval. I choose it instead of SciPy because it is easier to experiment with. It provides more bootstrap methods and natively supports resuing the results when changing bootstrap methods.</p><h3 id=simulation-study-of-coverages>Simulation Study of Coverages</h3><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_ci.png srcset="/blog/image/no_skip_smm/bootstrap_ci.png 1x,/blog/image/no_skip_smm/bootstrap_ci_2x.png 2x" alt="Recall of the procedure of testing the coverage."><figcaption class="text-center text-raven-500"><p>Recall of the procedure of testing the coverage.</p></figcaption></figure><p>If you use Arch directly, you will encounter the following exception</p><blockquote><p>Empirical probability used in bias correction is 0 or 1, and so bias cannot be corrected. This may occur in extremum statistics that are not well approximated by a normal in a finite sample.</p></blockquote><p>For us, it happens when a simulated sample $ \hat{X}_r^* $ excludes all occurrences of -15 lives and below. The maximum life loss is 14, so the survival probability of the first level is 100% for all the bootstrap resamples $ \hat{X}_{r,s}^* $. Patching Arch to handle the problem is fairly easy and fast.</p><p>We start by preliminarily checking the coverages of 95% confidence interval for several common bootstrap methods, basic, percentile, bias-corrected (BC), and bias-corrected and accelerated (BCa). 1000 bootstrap resamples repeating 1000 times is enough. It means that there are 1000 simulated samples $ \hat{X}_r^* $ and 1000 bootstrap resamples $ \hat{X}_{r,s}^* $ for each simulated samples. After obtaining a confidence interval for each of the 1000 true success rate $ \hat{\theta} $, we count numbers of confidence intervals that contain their corresponding success rate. I ran the test for levels from 1 to 3000 and plotted the graph below.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_ci_all_0.95.png srcset="/blog/image/no_skip_smm/bootstrap_ci_all_0.95.png 1x,/blog/image/no_skip_smm/bootstrap_ci_all_0.95_2x.png 2x" alt="The coverages of the 95% confidence intervals for 1000 bootstrap resamples repeating 1000 times."><figcaption class="text-center text-raven-500"><p>The coverages of the 95% confidence intervals for 1000 bootstrap resamples repeating 1000 times.</p></figcaption></figure><p>The dotted lines in the graph show the thresholds rejecting the null hypotheses that the corresponding measured coverages were the same as the nominal coverage of 95%. It was clear that the basic bootstrap gave different coverages. On the contrary, we could not reject the null hypotheses for the percentile, the BC, and the BCa bootstrap. You should be cautious that it did not mean that the coverages were the same as the nominal coverage. It is impossible to prove these null hypotheses. To tell if those methods gave the "same" coverages as the nominal coverage, we should perform <a href=https://en.wikipedia.org/wiki/Equivalence_test>equivalence tests</a>. From now on, we will exclude the basic bootstrap and increase the number of resamples and repeats.</p><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_ci_0.95.png srcset="/blog/image/no_skip_smm/bootstrap_ci_0.95.png 1x,/blog/image/no_skip_smm/bootstrap_ci_0.95_2x.png 2x" alt="The coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times."><figcaption class="text-center text-raven-500"><p>The coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</p></figcaption></figure><figure><table><thead><tr><th>Levels</th><th>Percentile</th><th>BC</th><th>BCa</th></tr></thead><tbody><tr><td>1000</td><td>94.9%</td><td>94.9%</td><td>95.0%</td></tr><tr><td>2000</td><td>94.8%</td><td>94.8%</td><td>95.2%</td></tr><tr><td>3000</td><td>94.8%</td><td>94.9%</td><td>95.1%</td></tr></tbody></table><figcaption>The coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/bootstrap_ci_0.99.png srcset="/blog/image/no_skip_smm/bootstrap_ci_0.99.png 1x,/blog/image/no_skip_smm/bootstrap_ci_0.99_2x.png 2x" alt="The coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times."><figcaption class="text-center text-raven-500"><p>The coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</p></figcaption></figure><figure><table><thead><tr><th>Levels</th><th>Percentile</th><th>BC</th><th>BCa</th></tr></thead><tbody><tr><td>1000</td><td>99.0%</td><td>99.0%</td><td>99.2%</td></tr><tr><td>2000</td><td>99.0%</td><td>99.0%</td><td>99.2%</td></tr><tr><td>3000</td><td>98.9%</td><td>99.0%</td><td>99.2%</td></tr></tbody></table><figcaption>The coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</figcaption></figure><p>They all performed similarly. I will not perform the equivalence tests here. Let us assume that they are equivalent to our use. We will use the BCa for the analysis.</p><h1 id=result>Result</h1><p>Combining the sample probability with the bootstrap result using the BCa for 40000 resamples gave the following result. As a bonus, I calculated all the probabilities for levels up to 3000. You should keep in mind that the probabilities of the confidence intervals shown below refer to the long-run frequencies of covering the true success rates for (hypothetical) repeated random sampling. A particular confidence interval either covers or does not cover the true success rate. After getting a sample and revealing the confidence interval, the cover rate of the confidence interval is only epistemic (one's ignorance). Further interpretations require a deeper understanding of the properties of bootstrapping. The definition of confidence intervals is loose in which it only concerns the cover rate. Some procedures give opposite widths or counter-intuitive intervals. Richard D. Morey, Rink Hoekstra, Jeffrey N. Rouder, Michael D. Lee, and Eric-Jan Wagenmakers explained the issues nicely in <em>The fallacy of placing confidence in confidence intervals</em><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. Below is the result:</p><table><thead><tr><th>Levels</th><th>Probability</th><th>95% CI</th><th>99% CI</th></tr></thead><tbody><tr><td>1000</td><td>61.6%</td><td>[15.7%, 85.4%]</td><td>[5.84%, 89.4%]</td></tr><tr><td>2000</td><td>58.9%</td><td>[8.38%, 86%]</td><td>[1.78%, 90.2%]</td></tr><tr><td>3000</td><td>56.3%</td><td>[3.96%, 86%]</td><td>[0.43%, 90.1%]</td></tr></tbody></table><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/pangas_probabilities.png srcset="/blog/image/no_skip_smm/pangas_probabilities.png 1x,/blog/image/no_skip_smm/pangas_probabilities_2x.png 2x" alt="Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities."><figcaption class="text-center text-raven-500"><p>Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.</p></figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/pangas_probabilities_simplified.png srcset="/blog/image/no_skip_smm/pangas_probabilities_simplified.png 1x,/blog/image/no_skip_smm/pangas_probabilities_simplified_2x.png 2x" alt="Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown."><figcaption class="text-center text-raven-500"><p>Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.</p></figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/pangas_probability_1000.png srcset="/blog/image/no_skip_smm/pangas_probability_1000.png 1x,/blog/image/no_skip_smm/pangas_probability_1000_2x.png 2x" alt="Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward."><figcaption class="text-center text-raven-500"><p>Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.</p></figcaption></figure><figure><img class="mx-auto leading-none" src=/blog/image/no_skip_smm/pangas_cdf_1000.png srcset="/blog/image/no_skip_smm/pangas_cdf_1000.png 1x,/blog/image/no_skip_smm/pangas_cdf_1000_2x.png 2x" alt="Cumulative probability of Panga successfully beating 1000 levels."><figcaption class="text-center text-raven-500"><p>Cumulative probability of Panga successfully beating 1000 levels.</p></figcaption></figure><h1 id=conclusion>Conclusion</h1><p>Undoubtedly, beating 1000 endless expert levels without skipping in Super Mario Marker 2 is hard for most players. It is not for ones without top-notch skills and strong determination. This article has quantified the difficulty by using the theory of the Markov process, bootstrapping, and one of the best players' data. You are likely to have even more questions about this challenge other than the success rate. In the future, we will delve deeper into the details in <a href=https://keithyipkw.github.io/blog/no_skip_smm_part_2/>part 2</a>.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>A tweet by Nintendo of America on 4 Sep, 2020. <a href=https://twitter.com/NintendoAmerica/status/1301928016004644864>https://twitter.com/NintendoAmerica/status/1301928016004644864</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Carpenter, J. and Bithell, J. (2000), Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians. Statist. Med., 19: 1141-1164. <a href=https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9>https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9</a>&lt;1141::AID-SIM479>3.0.CO;2-F <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Morey, R.D., Hoekstra, R., Rouder, J.N. et al. The fallacy of placing confidence in confidence intervals. Psychon Bull Rev 23, 103–123 (2016). <a href=https://doi.org/10.3758/s13423-015-0947-8>https://doi.org/10.3758/s13423-015-0947-8</a> <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article></div></div></div></main><footer class=footer><div class="ax-l-i max-w-6xl"><nav class="flex items-center justify-center"><a class="ml-3 first:ml-0 text-sm text-gray-600 hover:text-gray-800" href=/blog/blog/about/>About</a>
<a class="ml-3 first:ml-0 text-sm text-gray-600 hover:text-gray-800" href=/blog/blog/copyright/>Copyright</a></nav><div class="footer-copyright text-sm text-center text-gray-400 mt-4">&#169; 2022 STEM</div><div class="text-sm sm:text-xs text-center text-gray-400 mt-2">Powered by <a href="https://www.axiomtheme.com/?utm_source=theme-footer&utm_medium=website&utm_campaign=referral">Axiom</a></div></div></footer><script src="/blog/bundle.js?v=1661093204"></script></body></html>