<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-us">
  <title>STEM</title>
  <subtitle></subtitle>
  <id>https://keithyipkw.github.io/</id>
  <author>
    <name>STEM</name>
    <uri>https://keithyipkw.github.io/</uri>
  </author>
  <icon>https://keithyipkw.github.io/blog/image/brand/icon-1-1.png</icon>
  <logo>https://keithyipkw.github.io/blog/image/brand/icon-2-1.png</logo>
  <updated>2022-08-23T14:45:15Z</updated>
  <link rel="self" type="application/atom+xml" href="https://keithyipkw.github.io/blog/feed.atom" hreflang="en-us"/>
  <link rel="alternate" type="text/html" href="https://keithyipkw.github.io/blog/" hreflang="en-us"/>
  <entry>
    <title>Endless Expert Levels Without Skipping in SMM2 - Part 3</title>
    <author>
      <name>Keith Yip</name>
      <uri></uri>
    </author>
    <id>https://keithyipkw.github.io/blog/no_skip_smm_part_3/</id>
    <updated>2022-08-21T14:00:00Z</updated>
    <published>2022-08-21T14:00:00Z</published>
    <content type="html"><![CDATA[<p>In <a href="https://keithyipkw.github.io/blog/no_skip_smm_part_2/">part 2</a>, we improved the estimation of Panga's success rates of beating the various numbers of levels without skipping by utilizing the samples with a better estimator. Meanwhile, he completed more levels in his pursuit of beating 2000 levels, and it means that we have more samples for our estimation.</p>
<h1 id="new-data">New Data</h1>
<p>With the new total of 2573 samples, far more than the previous 726 samples, the estimation will be significantly more precise. As expected, the distribution of the samples is similar to that before.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/life_change_pmf.png"
srcset="/blog/image/no_skip_smm_3/life_change_pmf.png 1x,/blog/image/no_skip_smm_3/life_change_pmf_2x.png 2x"
alt="The overall sample distribution of Panga&#39;s life changes after beating a level. Values below 0.05% are visually enhanced.">
<figcaption class="text-center text-raven-500">
<p>The overall sample distribution of Panga's life changes after beating a level. Values below 0.05% are visually enhanced.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/life_change_pmf_group.png"
srcset="/blog/image/no_skip_smm_3/life_change_pmf_group.png 1x,/blog/image/no_skip_smm_3/life_change_pmf_group_2x.png 2x"
alt="The within-group sample distribution of Panga&#39;s life changes after beating a level. Values below 0.05% are visually enhanced.">
<figcaption class="text-center text-raven-500">
<p>The within-group sample distribution of Panga's life changes after beating a level. Values below 0.05% are visually enhanced.</p>
</figcaption>
</figure>

<p>Panga's best run tragically ended by a triple-triple shell jump level (performing three triple shell jumps) and reached 1906 levels. The level required him to spend more than 99 lives to beat. Encountering such a level was virtually a guaranteed end of a run for any player. His two subsequent runs also ended similarly by two triple shell jump levels.</p>
<div class="yt-container-16-9">
    <iframe src="https://www.youtube-nocookie.com/embed/hvbGMohca94?start=1389&end=1410" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<h1 id="validation-of-the-assumption-in-part-1">Validation of the Assumption in Part 1</h1>
<p>In <a href="https://keithyipkw.github.io/blog/no_skip_smm/">part 1</a>, we made the assumption</p>
<blockquote>
<p>The player performs practically the same when their number of lives is from 1 to 96 or 97 to 99.</p>
</blockquote>
<p>To roughly check the assumption, we will compare the life change probabilities by considering the uncertainty per each starting life. As shown in the following graphs, almost all values are either very close or within random variations. The remaining rare instances are still explainable by random chance. Comparing the uncensored values across all the graphs, the results are the same.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/life_change_pdf_97.png"
srcset="/blog/image/no_skip_smm_3/life_change_pdf_97.png 1x,/blog/image/no_skip_smm_3/life_change_pdf_97_2x.png 2x"
alt="Panga&#39;s life change probabilities after beating a level for 97 starting lives or those below. Each probability density function is calculated independently by using the binomial proportion.">
<figcaption class="text-center text-raven-500">
<p>Panga's life change probabilities after beating a level for 97 starting lives or those below. Each probability density function is calculated independently by using the binomial proportion.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/life_change_pdf_98.png"
srcset="/blog/image/no_skip_smm_3/life_change_pdf_98.png 1x,/blog/image/no_skip_smm_3/life_change_pdf_98_2x.png 2x"
alt="Panga&#39;s life change probabilities after beating a level for 98 starting lives or those below. Each probability density function is calculated independently by using the binomial proportion.">
<figcaption class="text-center text-raven-500">
<p>Panga's life change probabilities after beating a level for 98 starting lives or those below. Each probability density function is calculated independently by using the binomial proportion.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/life_change_pdf_99.png"
srcset="/blog/image/no_skip_smm_3/life_change_pdf_99.png 1x,/blog/image/no_skip_smm_3/life_change_pdf_99_2x.png 2x"
alt="Panga&#39;s life change probabilities after beating a level for 99 starting lives or those below. Each probability density function is calculated independently by using the binomial proportion.">
<figcaption class="text-center text-raven-500">
<p>Panga's life change probabilities after beating a level for 99 starting lives or those below. Each probability density function is calculated independently by using the binomial proportion.</p>
</figcaption>
</figure>

<p>A complete and proper examination should be a hypothesis test like an equivalence test on more important combinations of the starting lives and life changes. People, even scientists, often misinterpret the significance of overlapping probabilities and error bars<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. I will leave the test for those who are interested. If you do, be aware of false discoveries you may encounter when checking too many combinations.</p>
<h1 id="result">Result</h1>
<p>The sample probabilities and confidence intervals of Panga beating 1000, 2000, and 3000 levels plummet as the rare events surfaced. The sample probabilities hovered at 70% last time but become 15%, 4%, and 1% for the respective levels. The lower confidence limits are less than 3% to near impossible. The upper confidence limits drop from 91% to about 60%.</p>
<table>
<thead>
<tr>
<th>Levels</th>
<th>Probability</th>
<th>95% CI</th>
<th>99% CI</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>14.9%</td>
<td>[2.68%, 48.5%]</td>
<td>[1.44%, 63%]</td>
</tr>
<tr>
<td>2000</td>
<td>3.97%</td>
<td>[0.131%, 36.9%]</td>
<td>[0.0391%, 59.2%]</td>
</tr>
<tr>
<td>3000</td>
<td>1.06%</td>
<td>[0.00633%, 29.2%]</td>
<td>[0.00104%, 55.8%]</td>
</tr>
</tbody>
</table>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_probabilities.png"
srcset="/blog/image/no_skip_smm_3/pangas_probabilities.png 1x,/blog/image/no_skip_smm_3/pangas_probabilities_2x.png 2x"
alt="Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.">
<figcaption class="text-center text-raven-500">
<p>Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_probabilities_simplified.png"
srcset="/blog/image/no_skip_smm_3/pangas_probabilities_simplified.png 1x,/blog/image/no_skip_smm_3/pangas_probabilities_simplified_2x.png 2x"
alt="Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.">
<figcaption class="text-center text-raven-500">
<p>Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_probability_1000.png"
srcset="/blog/image/no_skip_smm_3/pangas_probability_1000.png 1x,/blog/image/no_skip_smm_3/pangas_probability_1000_2x.png 2x"
alt="Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.">
<figcaption class="text-center text-raven-500">
<p>Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_cdf_1000.png"
srcset="/blog/image/no_skip_smm_3/pangas_cdf_1000.png 1x,/blog/image/no_skip_smm_3/pangas_cdf_1000_2x.png 2x"
alt="Cumulative probability of Panga successfully beating 1000 levels.">
<figcaption class="text-center text-raven-500">
<p>Cumulative probability of Panga successfully beating 1000 levels.</p>
</figcaption>
</figure>

<p>As a reminder, the inference used here is frequentist. The probabilities of the confidence intervals and cumulative probabilities refer to the corresponding (hypothetical) long-run frequencies that the sampling and estimation procedure are correct. For example, a 99% confidence interval means that 99% of such confidence intervals generated by the confidence procedure cover the true success rate. Whether an interval covers the true success rate is deterministic yes or no but is unknown to us.</p>
<h1 id="reaper-levels">Reaper Levels</h1>
<p>Dodging run-ending levels are hard. It is like avoiding getting a one when you try throwing a dice many times. You may be able to throw 10 times, but if you keep throwing, you will eventually get a one. The exact probabilities of those happening are easy to calculate. Suppose that $ p $ is the probability of the outcome in an event, the probability of not having the outcome in $ n $ events is the complement of $ p $, raised to the power of $ n $:</p>

$$
P(n) = (1 - p)^n 
$$

<p>Let us only consider the sample probability of Panga encountering a run-ending level first for simplicity. It is 3 out of 2573 which is 0.117%. Plugging the number into the formula, we have the values shown in the following graph:</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_probabilities_avoiding_reaper.png"
srcset="/blog/image/no_skip_smm_3/pangas_probabilities_avoiding_reaper.png 1x,/blog/image/no_skip_smm_3/pangas_probabilities_avoiding_reaper_2x.png 2x"
alt="The probabilities of Panga doging a run-ending level when playing certain number of levels.">
<figcaption class="text-center text-raven-500">
<p>The probabilities of Panga doging a run-ending level when playing certain number of levels.</p>
</figcaption>
</figure>

<p>This kind of level alone is enough to bring the success rate of beating 1000 levels down to 31% and 2000 levels down to 9.7%.</p>
<p>The true probability of encountering a run-ending level is unknown as usual, so we should apply the formula to other possible values too. For instance, the upper 99% confidence limit of an encounter is 0.460%. It greatly lowers the success rate of beating 1000 and 2000 levels to 0.99% and 0.0099% respectively.</p>
<h1 id="precision">Precision</h1>
<p>The estimations sometimes can be off very much as shown in <a href="https://keithyipkw.github.io/blog/no_skip_smm_part_2/">part 2</a> and here. The estimated Panga's success rate of beating 1000 levels was previously 61.6% but is 14.9% here. The estimated success rate is sensitive to small errors in the estimated probabilities of encountering some levels, like the reaper levels. Recalls that we calculated the estimated success rate by multiplying the state vector to the transition matrix thousands of times. A small error accumulated into a huge error in the final estimation. Randomness was the first source of the error. Discreteness in the sample space, which was the probability of encountering a level causing a certain life change, was the second source. Such a sample probability could only be one of the finitely many values regardless of random chance. It might be $ \frac{1}{2573} $, $ \frac{2}{2573} $, $ \frac{3}{2573} $, or so on, but never $ \frac{0.5}{2573} $. The resolution might not be enough.</p>
<p>To illustrate the effect, we add one sample of each life change to the existing 2573 samples and calculate the corresponding rates:</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_success_rates_1000_levels_adding_1_sample.png"
srcset="/blog/image/no_skip_smm_3/pangas_success_rates_1000_levels_adding_1_sample.png 1x,/blog/image/no_skip_smm_3/pangas_success_rates_1000_levels_adding_1_sample_2x.png 2x"
alt="Panga&#39;s success rates of beating 1000 levels after adding a sample of various life changes.">
<figcaption class="text-center text-raven-500">
<p>Panga's success rates of beating 1000 levels after adding a sample of various life changes.</p>
</figcaption>
</figure>

<p>The original sample success rate is about 15%. Adding a sample with a small life change does not affect the rate much. However, when the life loss increases, the difference grows substantially. At the extreme, it drops from 15% to 10%.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_success_rates_adding_1_sample.png"
srcset="/blog/image/no_skip_smm_3/pangas_success_rates_adding_1_sample.png 1x,/blog/image/no_skip_smm_3/pangas_success_rates_adding_1_sample_2x.png 2x"
alt="Panga&#39;s success rates of beating various numbers of levels after adding a sample of various life changes in a semi-log scale. The spaces between the lines denoting the success rates increase as the level of greater life loss is more impactful to the estimations.">
<figcaption class="text-center text-raven-500">
<p>Panga's success rates of beating various numbers of levels after adding a sample of various life changes in a semi-log scale. The spaces between the lines denoting the success rates increase as the level of greater life loss is more impactful to the estimations.</p>
</figcaption>
</figure>

<p>The effect grows as the number of levels increases. At 1000 levels, the rate after adding a 99-life loss sample drops from 15% to 10%, which is one-third of 15%. At 3000 levels, it drops from 1% to 0.3%, which is two-thirds of 1%. In short, the estimations are sensitive to the discreteness and the randomness of some measurements. When everything compounds, they seriously affect the precision of our estimations.</p>
<p>What about the confidence intervals? Can they rescue us? The answer is &quot;kind of&quot;. They help by stating intervals instead of point estimations but not in the way we demand. There is only one basic property of a confidence interval. Its construction procedure needs to generate intervals covering the true value with the specified rate in long run. The closeness between the ends of the intervals and the true value is not a concern, although some confidence intervals handle it somewhat nicely. The confidence interval calculated by Z-score for normal distributions is an example. If precision is a concern, it is better to directly estimate the precision, use an appropriate confidence procedure, or use other intervals, e.g. <a href="https://en.wikipedia.org/wiki/Credible_interval">Bayesian credible intervals</a>. Bayesian is another popular inference besides frequentist. When a credible interval says that the probability of a value falls into the interval, it means the probability among all possible parameters. That is what people usually have in mind. Credible intervals operate on likelihoods, so they are more connected to precision.</p>
<p>Despite all these, bootstraps are generally quite good at reflecting precision when there are sufficient samples. They have some Bayesian properties<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> and so produce intervals approximating credible intervals with a caveat. In &quot;The Bayesian Bootstrap&quot;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, just as Rubin (1981) stated:</p>
<blockquote>
<p>... is it reasonable to use a model specification that effectively assumes all possible distinct values of X have been observed? Both the BB and the bootstrap operate under this assumption. In some cases inferences may be insensitive to this assumption but not always. (p. 133)</p>
</blockquote>
<p>We happened to encounter the perfect example in part 1, having no samples on the most influential non-zero parameter. However, this time we have samples for the 99-life loss. Even though we have no samples for other very large life losses, it should be fine. The distributions of the samples clearly show right-censored bell shapes. Extrapolating the trend to very large life losses indicates that the true probabilities are likely to be vanishingly small. They are different from the special 99 life-loss, which includes all even harder levels. Our new confidence interval now can reflect the precision.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/life_change_pmf_group_97.png"
srcset="/blog/image/no_skip_smm_3/life_change_pmf_group_97.png 1x,/blog/image/no_skip_smm_3/life_change_pmf_group_97_2x.png 2x"
alt="The within-group sample distribution of Panga&#39;s life changes after beating a level with a start lives less than 97 in linear scales. The distribution shows a right-censored bell shape. Encountering a level causing a huge life loss other than 99 lives should be vanishingly rare.">
<figcaption class="text-center text-raven-500">
<p>The within-group sample distribution of Panga's life changes after beating a level with a start lives less than 97 in linear scales. The distribution shows a right-censored bell shape. Encountering a level causing a huge life loss other than 99 lives should be vanishingly rare.</p>
</figcaption>
</figure>

<h1 id="to-infinity">To Infinity</h1>
<p>The graph of the sample and confidence intervals of Panga's success rate of beating various levels looks suspiciously like exponential decays. Plotting it with a log scale on the y-axis reveals the long-term trend of the success rate, which is linear in a log scale.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_probabilities_simplified_log.png"
srcset="/blog/image/no_skip_smm_3/pangas_probabilities_simplified_log.png 1x,/blog/image/no_skip_smm_3/pangas_probabilities_simplified_log_2x.png 2x"
alt="Probabilities of Panga successfully beating various numbers of levels in a log scale. The decays are exponential and so linear in log scale.">
<figcaption class="text-center text-raven-500">
<p>Probabilities of Panga successfully beating various numbers of levels in a log scale. The decays are exponential and so linear in log scale.</p>
</figcaption>
</figure>

<p>It suggests the following asymptotic behaviors. Let</p>
<ul>
<li>$ P(k) $ be the probability of beating $ k $ levels,</li>
<li>$ a $, $ b $ be some constants,</li>
<li>$ \lambda $ be the constant decay rate,</li>
</ul>
<p>and with</p>
<ul>
<li>$ \sim $ denoting &quot;is asymptotically equivalent to&quot;,</li>
<li>$ \to $ denoting &quot;tends to&quot;,</li>
<li>$ ak+b $ being the linear relationship.</li>
</ul>
<p>We have</p>

$$
\begin{aligned}

P(k) &\sim e^{ak+b} \\
P(k+1) &\sim \lambda P(k) \quad \text{as } k \to \infty \\

\end{aligned}
$$

<p>The constant decay rate comes from a deeper cause. In <a href="https://keithyipkw.github.io/blog/no_skip_smm/">part 1</a>, we learnt that by formulating the problem as a Markov process, we could calculate $ P(k+1) $ from the state vector $ \vec s_k $ and transition matrix $ \mathbf T_{100\times100} $:</p>

$$
\begin{aligned}
\vec s_k &= \begin{bmatrix}
P_s(k,\textcolor{red}{♥}=0) & P_s(k,\textcolor{red}{♥}=1) & \ldots & P_s(k,\textcolor{red}{♥}=99)
\end{bmatrix} \\ \\

\mathbf T &= \begin{bmatrix}
1 & 0 & \ldots & 0 \\
P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=-1) & P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=0) & \ldots & P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=98) \\
\vdots & \vdots & \ddots & \vdots \\
P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=-99) & P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=-98) & \ldots & P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=0) \\
\end{bmatrix} \\ \\

\vec s_{k+1} &= \vec s_k \mathbf T \\ \\

P(k) &= 1 - P_s(k,\textcolor{red}{♥}=0) \\
&= \sum_{\textcolor{red}{♥}=1}^{99} P_s(k,\textcolor{red}{♥})

\end{aligned}
$$

<p>Partitioning the state vector gives the non-absorbing state (survival state) vector $ \vec r $ and the transition matrix gives the corresponding transition matrix $ \mathbf U_{99\times99} $:</p>

$$
\begin{aligned}

\vec s_{k} &= \begin{bmatrix}
P_s(k,\textcolor{red}{♥}=0) & \vec r_k
\end{bmatrix} \\

\mathbf T &= \left[ \begin{array}{c|c}
1 & \begin{matrix}
    0 & \ldots & 0
    \end{matrix} \\
\hline
    \begin{matrix}
    P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=-1) \\
    \vdots \\
    P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=-99)
    \end{matrix} & \Large \mathbf U
\end{array} \right]

\end{aligned}
$$

<p>Then we have:</p>

$$
\begin{align}

P(k) &= \sum_{i=1}^{99} r_{k,i} \notag \\
\vec r_{k+1} &= \vec r_k \mathbf U \label{eq1}

\end{align}
$$

<p>The asymptotic process that a state is similar to its previous state causes exponential decay of the success rate:</p>

$$
\begin{aligned}

\vec r_{k+1} &\sim \lambda \vec r_k \quad \text{as } k \to \infty \\
P(k+1) &= \sum_{i=1}^{99} r_{k+1,i} \\
&\sim \sum_{i=1}^{99} \lambda r_{k,i} \\
&= \lambda P(k)

\end{aligned}
$$

<p>A simple way to calculate the decay rate is to iterate equation $ \ref{eq1} $ until the value converges. The decay rate quickly converges to 99.87%.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_sample_success_rate_decay.png"
srcset="/blog/image/no_skip_smm_3/pangas_sample_success_rate_decay.png 1x,/blog/image/no_skip_smm_3/pangas_sample_success_rate_decay_2x.png 2x"
alt="The decay rate of Panga&#39;s sample success rate in each iteration. It settles down at about 600 levels.">
<figcaption class="text-center text-raven-500">
<p>The decay rate of Panga's sample success rate in each iteration. It settles down at about 600 levels.</p>
</figcaption>
</figure>

<p>There is another interesting but slightly more complicated way to calculate the decay rate. $ \vec r_k $ are similar to each other and differ by a scale. If we normalize it, take the limit, and substitute it into $ \ref {eq1} $, we have a new equation:</p>

$$

\vec r \mathbf U = \lambda \vec r

$$

<p>where</p>

$$

\vec r = \lim_{k \to \infty}\frac{\vec r_{k}}{\sum_{i=1}^{99} r_{k,i}}

$$

<p>$ \vec r $ is an <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">eigenvector</a> of $ \mathbf U $ and $ \lambda $ is the corresponding eigenvalue. The equation means that when the vector multiplies the matrix, the result is the same as it multiplying with a constant. Such an equation is so common in science and engineering that most linear algebra software provides direct functionalities to solve it. However, the complication is after that. There are as many pairs of eigenvector and eigenvalue as the dimension of the matrix, which is 99 pairs here. For Panga's transition matrix, there is one sensible pair that</p>
<ul>
<li>has a real eigenvector,</li>
<li>all elements in the eigenvector and the eigenvalue have the same sign.</li>
</ul>
<p>Here, we only need to pick the pair of eigenvector and eigenvalue with all the elements being positive and the eigenvalue being less than one after normalization. Either way gives us the same solution.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_3/pangas_lives_limit_distribution.png"
srcset="/blog/image/no_skip_smm_3/pangas_lives_limit_distribution.png 1x,/blog/image/no_skip_smm_3/pangas_lives_limit_distribution_2x.png 2x"
alt="The normalized limiting distribution of Panga&#39;s lives $ \vec r $ after beating a level. Having a healthy amount of lives was one of his advantages.">
<figcaption class="text-center text-raven-500">
<p>The normalized limiting distribution of Panga's lives $ \vec r $ after beating a level. Having a healthy amount of lives was one of his advantages.</p>
</figcaption>
</figure>

<h1 id="conclsuion">Conclsuion</h1>
<p>With the new samples, we could better validate the assumption we made before that the player performs practically the same regardless of their starting lives and improve the estimation of Panga's success rate. Our analysis revealed the key to performing well in the no-skip endless challenge other than being generally good at the game. Because Nintendo's level rating system is imprecise for freshly uploaded levels made by and played by other top players, you must prepare for those levels too. We also experienced a perfect example of the limitation of bootstraps. They did not give confidence intervals that had the Bayesian properties and fell back to the minimal frequentist interpretation when there were insufficient samples for sensitive parameters. Perhaps in the future, we will try a Bayesian inference.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Krzywinski, M., Altman, N. Error bars. Nat Methods10, 921–922 (2013). https://doi.org/10.1038/nmeth.2659 <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Donald B. Rubin.  &quot;The Bayesian Bootstrap.&quot; Ann. Statist. 9 (1) 130 - 134, January, 1981. https://doi.org/10.1214/aos/1176345338 <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Trevor Hastie, Robert Tibshirani, Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second Edition, February 2009. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>]]></content>
  </entry>
  <entry>
    <title>Endless Expert Levels Without Skipping in SMM2 - Part 2</title>
    <author>
      <name>Keith Yip</name>
      <uri></uri>
    </author>
    <id>https://keithyipkw.github.io/blog/no_skip_smm_part_2/</id>
    <updated>2022-08-23T13:45:00Z</updated>
    <published>2022-03-14T14:20:00Z</published>
    <content type="html"><![CDATA[<p>In <a href="https://keithyipkw.github.io/blog/no_skip_smm/">part 1</a>, we learned how to use bootstraps to calculate confidence intervals of Panga's success rates of beating the various numbers of levels without skipping. We threw away the censored samples that were 31% of the samples for simplicity. This time, we will use all of them to infer the success rates and confidence intervals.</p>
<h1 id="a-better-estimator">A Better Estimator</h1>
<p>The maximum number of lives, 99, limits the maximum life change after beating a level in addition to the maximum 3-life gain. For a player having a starting life from 1 to 96 before playing a level, there is no censorship in the observation of their life change after beating the level. However, if they have a starting life from 97 to 99, the observed life change is capped. When mixing these samples with the former samples, we need to take care of this <a href="https://en.wikipedia.org/wiki/Censoring_(statistics)">right censoring</a> problem.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator">Kaplan–Meier estimator</a> is our solution. We will group the probabilities of life changes that the censoring does not skew the estimation. By applying the same technique to each group containing multiple life changes and ignoring the censored samples useless to the subgroups, we will be able to estimate the probabilities of each subgroup. Repeating the steps will eventually give us the probabilities of each life change.</p>
<p>Because we are dealing with a right censoring, we will estimate the probabilities from the most life loss toward the most life gain. For life losses, we will simply count the proportion for each one and group the remaining for life gain greater than or equal to 0. Doing so will allow us to include the censored samples when estimating the probabilities of each group. After throwing away the samples with 99 starting lives and negative life changes, we will count the proportion for 0-life change and group the remaining for 1 or more life gain. The overall proportion for 0-life change will be the product of the previous proportion for 0 or more life gain and the current proportion for 0-life change. By repeating a similar procedure for 1 to 3-life gains, we will get the probabilities for all life changes using all the samples.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/estimator_example_1a.png"
srcset="/blog/image/no_skip_smm_2/estimator_example_1a.png 1x,/blog/image/no_skip_smm_2/estimator_example_1a_2x.png 2x"
alt="For example, a player having 97 starting lives gained 1 life once and 2 lives 6 times. In part 1, we threw away these censored samples and only relied on the samples with 1 to 96 starting lives.">
<figcaption class="text-center text-raven-500">
<p>For example, a player having 97 starting lives gained 1 life once and 2 lives 6 times. In part 1, we threw away these censored samples and only relied on the samples with 1 to 96 starting lives.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/estimator_example_1d.png"
srcset="/blog/image/no_skip_smm_2/estimator_example_1d.png 1x,/blog/image/no_skip_smm_2/estimator_example_1d_2x.png 2x"
alt="To calculate the probability of 1-life gain, we group the samples into 1-life gain and other life gains. It is the former divided by the sum of both, which is $ \frac{1 &#43; 2}{1 &#43; 2 &#43; 3 &#43; 6 &#43; 2} = 21\%$. That of 2 or more life gain is $ 100\% - 21\% = 79\% $">
<figcaption class="text-center text-raven-500">
<p>To calculate the probability of 1-life gain, we group the samples into 1-life gain and other life gains. It is the former divided by the sum of both, which is $ \frac{1 + 2}{1 + 2 + 3 + 6 + 2} = 21\%$. That of 2 or more life gain is $ 100\% - 21\% = 79\% $</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/estimator_example_1e.png"
srcset="/blog/image/no_skip_smm_2/estimator_example_1e.png 1x,/blog/image/no_skip_smm_2/estimator_example_1e_2x.png 2x"
alt="Then we throw away the samples with 97 start lives and 1-life gain. The probabilty of 2-life gain is $ \frac{3}{3 &#43; 5} \times 79\% = 30\% $. That of 3-life gain is $ 79\% - 30\% = 49\% $.">
<figcaption class="text-center text-raven-500">
<p>Then we throw away the samples with 97 start lives and 1-life gain. The probabilty of 2-life gain is $ \frac{3}{3 + 5} \times 79\% = 30\% $. That of 3-life gain is $ 79\% - 30\% = 49\% $.</p>
</figcaption>
</figure>

<p>Formally, let</p>
<ul>
<li>$ \textcolor{red}{♥} \in \set{0,1,2,\ldots,99} $ be the number of starting lives.</li>
<li>$ \Delta\textcolor{red}{♥} \in \set{-99, -98, -97, \ldots, 3} $ be the life change.</li>
<li>$ X(c) $ be the number of samples that condition $ c $ is true.</li>
<li>$ P(\Delta\textcolor{red}{♥}) $ be the probability of life change of $ \Delta\textcolor{red}{♥} $ without considering the censoring.</li>
</ul>
<p>The probabilities of negative life changes, $ m $, are:</p>

$$
\begin{aligned}
P(\Delta\textcolor{red}{♥}=m) &= \frac{X(1\le\textcolor{red}{♥}\le99,\Delta\textcolor{red}{♥}=m)}{X(1\le\textcolor{red}{♥}\le99,-99\le\Delta\textcolor{red}{♥}\le3)}, & -99 \le m < 0
\end{aligned}
$$

<p>The probability of all non-negative life changes is the complement:</p>

$$
P(\Delta\textcolor{red}{♥}\ge 0) = 1 - P(\Delta\textcolor{red}{♥}<0)
$$

<p>The probability of 0-life change is:</p>

$$
P(\Delta\textcolor{red}{♥}=0) = P(\Delta\textcolor{red}{♥}\ge 0) \frac{X(1\le\textcolor{red}{♥}\le98,\Delta\textcolor{red}{♥}=0)}{X(1\le\textcolor{red}{♥}\le98,0\le\Delta\textcolor{red}{♥}\le3)} 
$$

<p>The probability of 1 or more life changes is:</p>

$$

P(\Delta\textcolor{red}{♥}\ge 1) = P(\Delta\textcolor{red}{♥}\ge0) - P(\Delta\textcolor{red}{♥}=0)

$$

<p>Repeating the same logic gives the remaining probabilities:</p>

$$
\begin{aligned}

P(\Delta\textcolor{red}{♥}\ge 1) &= P(\Delta\textcolor{red}{♥}\ge0) - P(\Delta\textcolor{red}{♥}=0) \\

P(\Delta\textcolor{red}{♥}=1) &= P(\Delta\textcolor{red}{♥}\ge 1) \frac{X(1\le\textcolor{red}{♥}\le97,\Delta\textcolor{red}{♥}=1)}{X(1\le\textcolor{red}{♥}\le97,1\le\Delta\textcolor{red}{♥}\le3)} \\

P(\Delta\textcolor{red}{♥}\ge 2) &= P(\Delta\textcolor{red}{♥}\ge1) - P(\Delta\textcolor{red}{♥}=1) \\

P(\Delta\textcolor{red}{♥}=2) &= P(\Delta\textcolor{red}{♥}\ge 2) \frac{X(1\le\textcolor{red}{♥}\le96,\Delta\textcolor{red}{♥}=2)}{X(1\le\textcolor{red}{♥}\le96,2\le\Delta\textcolor{red}{♥}\le3)} \\

P(\Delta\textcolor{red}{♥}=3) &= P(\Delta\textcolor{red}{♥}\ge2) - P(\Delta\textcolor{red}{♥}=2) \\

\end{aligned}
$$

<h1 id="sampling-process">Sampling Process</h1>
<p>The sampling process is different from independently drawing samples from a distribution with replacements that non-parametric bootstraps work perfectly. The censoring of a sample depends on the starting life where the life is the sum of the starting lives and the life change of its predecessor. The probabilities of life changes correlate with the censoring. The better the player is, the heavier the censoring there is. It is not obvious if a simple non-parametric bootstrap will work. Instead, we use parametric bootstraps with a model of the sampling process to calculate the confidence intervals.</p>
<p>Like the non-parametric counterparts, the BCa version is the least restrictive and generally provides the most accurate confidence intervals. However, it is more complicated than the non-parametric counterpart, especially when dealing with correlated samples. The details can be found in section 4 of <em>The Automatic Construction of Bootstrap Confidence Intervals</em> by Bradley Efron and Balasubramanian Narasimhan<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Rather than jumping into another deep rabbit hole immediately, we will spend some time studying the problem first by examining the actual differences between the samples obtained by different sampling processes. Below are some simulations of the original sampling, a multinomial sampling, and a non-parametric resampling process. Including the resampling process will help us to decide if a BCa bootstrap is necessary. There were 1058 samples of levels from part 1. The life change probabilities given by the Kaplan-Meier estimator formed a multinomial distribution. It was the basis of the simulations for the original sampling and multinomial sampling.</p>
<p>The original sampling process drew a sequence of 1058 levels from the distribution with replacements. It assigned 15 starting lives to the first level. The starting lives for each successive level were the sum of the current start lives and the life change. They were less than or equal to 99. Whenever a starting life fell below 1, it became 15 as if a run restarted. The Kaplan–Meier estimator gave an estimate of the life change probabilities for a new set of samples. In the simulation, I repeated the process 1,000,000 times and obtained distributions of the simulated estimates.</p>
<p>In the multinomial sampling, drawing 1058 levels was the whole process. There was no censoring. The process was simple enough that a formula for the exact distributions existed. I did not have to run a simulation.</p>
<p>The resampling was the same as those in non-parametric bootstraps. It directly drew new sets of 1058 samples from the 1058 samples with replacements. Like the other simulation, I repeated it 1,000,000 times and obtained the corresponding distributions again.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/life_change_pmf.png"
srcset="/blog/image/no_skip_smm_2/life_change_pmf.png 1x,/blog/image/no_skip_smm_2/life_change_pmf_2x.png 2x"
alt="The probabilties of Panga&#39;s life change after beating a level estimated by the Kaplan–Meier estimator.">
<figcaption class="text-center text-raven-500">
<p>The probabilties of Panga's life change after beating a level estimated by the Kaplan–Meier estimator.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/-1_life_gain_cdf.png"
srcset="/blog/image/no_skip_smm_2/-1_life_gain_cdf.png 1x,/blog/image/no_skip_smm_2/-1_life_gain_cdf_2x.png 2x"
alt="The cumulative distributions of the probabilities of 1 life loss by the simulated sampling processes. The censoring did not affect life loss, so the distributions were the same.">
<figcaption class="text-center text-raven-500">
<p>The cumulative distributions of the probabilities of 1 life loss by the simulated sampling processes. The censoring did not affect life loss, so the distributions were the same.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/3_life_gain_cdf.png"
srcset="/blog/image/no_skip_smm_2/3_life_gain_cdf.png 1x,/blog/image/no_skip_smm_2/3_life_gain_cdf_2x.png 2x"
alt="The cumulative distributions of the probabilities of 3-life gain by the simulated sampling processes. That by the original sampling and the resampling were very close, unlike the multinomial sampling.">
<figcaption class="text-center text-raven-500">
<p>The cumulative distributions of the probabilities of 3-life gain by the simulated sampling processes. That by the original sampling and the resampling were very close, unlike the multinomial sampling.</p>
</figcaption>
</figure>

<p>The distributions of the probabilities of the non-negative life changes calculated by the original sampling process and the resamplings were considerably close. You would probably guess that the distributions of the sample success rates would behave the same. Simulating 400,000 times gave the following results. That by the resampling was indeed very close to that by the original sampling, but that by the multinomial sampling was even closer. Using only certain numbers of initial samples, e.g., 25% or 50%, gave similar results. A rigorous proof should cover all distributions of life change probabilities, but for now, let us assume that all sampling processes are practically the same.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/beat_1000_cdf.png"
srcset="/blog/image/no_skip_smm_2/beat_1000_cdf.png 1x,/blog/image/no_skip_smm_2/beat_1000_cdf_2x.png 2x"
alt="The cumulative distributions of the success rates of beating 1000 levels. The line for the original sample was behind that for the multinomial sampling. All were very close. There was only a small bias in that by the resampling.">
<figcaption class="text-center text-raven-500">
<p>The cumulative distributions of the success rates of beating 1000 levels. The line for the original sample was behind that for the multinomial sampling. All were very close. There was only a small bias in that by the resampling.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/beat_1000_cdf_zoom.png"
srcset="/blog/image/no_skip_smm_2/beat_1000_cdf_zoom.png 1x,/blog/image/no_skip_smm_2/beat_1000_cdf_zoom_2x.png 2x"
alt="A zoomed section of the previous graph. The distributions by the original sampling and the multinomial sampling were virtually indistinguishable. That by the resampling was slightly different but still practically the same.">
<figcaption class="text-center text-raven-500">
<p>A zoomed section of the previous graph. The distributions by the original sampling and the multinomial sampling were virtually indistinguishable. That by the resampling was slightly different but still practically the same.</p>
</figcaption>
</figure>

<h1 id="bootstrap-method">Bootstrap Method</h1>
<p>If you pay close attention to the result in part 1, you will notice that the confidence limits were weird. The upper confidence limits were non-sensible after 1200 levels. They rose slightly after 1200 levels. The state of a run can never transit from ended to in-progress, so the success rates must be monotonic decreasing. A similar logic applies to the bootstrap replications of the success rates too. The distribution of the bootstrap replications is an ordered set of success rates. Although the success rates for different life change probabilities may reorder in the next number of levels, an element in an ordered set of the success rates are monotonic decreasing. The lower confidence limits plateaued between 1200 to 1500 levels. It probably traced back to a sudden increment of around 750 levels suggested by the coverage simulation. The randomness in the bootstrap did not cause the weirdness. We used a large enough number of bootstrap replications that the confidence limits converged. Running the bootstrap with different random seeds would give the same result. These issues happened because we used the BCa bootstrap to calculate with extreme confidence limits. The BCa bootstrap was unstable when the confidence intervals were larger than 95%<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/part1_pangas_probabilities_simplified.png"
srcset="/blog/image/no_skip_smm_2/part1_pangas_probabilities_simplified.png 1x,/blog/image/no_skip_smm_2/part1_pangas_probabilities_simplified_2x.png 2x"
alt="The confidence limits from part 1. The upper confidence limits rose after 1200 levels while the lower confidence limits plateaued between 1200 to 1500 levels.">
<figcaption class="text-center text-raven-500">
<p>The confidence limits from part 1. The upper confidence limits rose after 1200 levels while the lower confidence limits plateaued between 1200 to 1500 levels.</p>
</figcaption>
</figure>

<p>This time we check the results of different bootstrap methods instead of blindly going with a BCa bootstrap. I calculated the confidence limits of the success rates of beating 1 to 3000 levels, the cumulative distribution functions of the success rates of beating 1000 levels calculated by the three bootstrap methods with 400,000 resamples, and a simulation study of the coverages of the confidence intervals. Below is the results:</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/bootstrap_method_comparision.png"
srcset="/blog/image/no_skip_smm_2/bootstrap_method_comparision.png 1x,/blog/image/no_skip_smm_2/bootstrap_method_comparision_2x.png 2x"
alt="The confidence intervals calculated by the three bootstrap methods.">
<figcaption class="text-center text-raven-500">
<p>The confidence intervals calculated by the three bootstrap methods.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/bootstrap_method_comparision_1000.png"
srcset="/blog/image/no_skip_smm_2/bootstrap_method_comparision_1000.png 1x,/blog/image/no_skip_smm_2/bootstrap_method_comparision_1000_2x.png 2x"
alt="The cumulative distributions of the success rate of beating 1000 levels calculated by the three bootstrap methods.">
<figcaption class="text-center text-raven-500">
<p>The cumulative distributions of the success rate of beating 1000 levels calculated by the three bootstrap methods.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/bootstrap_ci_0.95.png"
srcset="/blog/image/no_skip_smm_2/bootstrap_ci_0.95.png 1x,/blog/image/no_skip_smm_2/bootstrap_ci_0.95_2x.png 2x"
alt="The coverages of 95% confidence intervals by 4000 simulations.">
<figcaption class="text-center text-raven-500">
<p>The coverages of 95% confidence intervals by 4000 simulations.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/bootstrap_ci_0.99.png"
srcset="/blog/image/no_skip_smm_2/bootstrap_ci_0.99.png 1x,/blog/image/no_skip_smm_2/bootstrap_ci_0.99_2x.png 2x"
alt="The coverages of 99% confidence intervals by 4000 simulations.">
<figcaption class="text-center text-raven-500">
<p>The coverages of 99% confidence intervals by 4000 simulations.</p>
</figcaption>
</figure>

<p>The same issue showed up again. In comparison, the BC bootstrap performed better than the BCa bootstrap. It gave similar results most of the time but without instability. Combining it with the result from the previous section that the non-parametric resampling process approximated the original sampling process well, we can conclude that the parametric BC bootstrap is the way to go. We have just come full circle.</p>
<h1 id="result">Result</h1>
<p>Panga did better in this larger set of samples than the subset in part 1. He gained 0.287 lives on average after beating a level. Below is the results. As a reminder, the inference used here is frequentist. The probabilities of the confidence intervals and cumulative probability refer to the corresponding (hypothetical) long-run frequencies of those being true for the same sampling and estimation procedure.</p>
<table>
<thead>
<tr>
<th>Levels</th>
<th>Probability</th>
<th>95% CI</th>
<th>99% CI</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>71.2%</td>
<td>[36.7%, 88.1%]</td>
<td>[24.7%, 91.3%]</td>
</tr>
<tr>
<td>2000</td>
<td>70%</td>
<td>[27.6%, 88.1%]</td>
<td>[14.4%, 91.3%]</td>
</tr>
<tr>
<td>3000</td>
<td>68.8%</td>
<td>[20.7%, 88%]</td>
<td>[8.41%, 91.3%]</td>
</tr>
</tbody>
</table>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/pangas_probabilities.png"
srcset="/blog/image/no_skip_smm_2/pangas_probabilities.png 1x,/blog/image/no_skip_smm_2/pangas_probabilities_2x.png 2x"
alt="Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.">
<figcaption class="text-center text-raven-500">
<p>Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/pangas_probabilities_simplified.png"
srcset="/blog/image/no_skip_smm_2/pangas_probabilities_simplified.png 1x,/blog/image/no_skip_smm_2/pangas_probabilities_simplified_2x.png 2x"
alt="Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.">
<figcaption class="text-center text-raven-500">
<p>Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/pangas_probability_1000.png"
srcset="/blog/image/no_skip_smm_2/pangas_probability_1000.png 1x,/blog/image/no_skip_smm_2/pangas_probability_1000_2x.png 2x"
alt="Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.">
<figcaption class="text-center text-raven-500">
<p>Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm_2/pangas_cdf_1000.png"
srcset="/blog/image/no_skip_smm_2/pangas_cdf_1000.png 1x,/blog/image/no_skip_smm_2/pangas_cdf_1000_2x.png 2x"
alt="Cumulative probability of Panga successfully beating 1000 levels.">
<figcaption class="text-center text-raven-500">
<p>Cumulative probability of Panga successfully beating 1000 levels.</p>
</figcaption>
</figure>

<p>Compared to the result in part 1, the sample probabilities of beating 1000, 2000, and 3000 levels are 10% more. The upper confidence limits stay approximately the same while the bottom confidence limits shift upwards substantially. While most players struggle to beat 10 levels, Panga, being one of the best players in Super Mario Maker, only needs a few trials to have a decent chance to beat 1000 levels or even 2000 levels.</p>
<h1 id="conclusion">Conclusion</h1>
<p>This time we have squeezed all information out of our precious samples. As of the time I am writing this, Panga's best run had already passed 1000 levels, ended at 1906 levels<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. He is devoted to reaching 2000 levels. It means that we have more samples to reduce the uncertainty in our estimations. Besides, there are other interesting things to discuss in the <a href="https://keithyipkw.github.io/blog/no_skip_smm_part_3/">part 3</a>.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Bradley Efron &amp; Balasubramanian Narasimhan (2020) The Automatic Construction of Bootstrap Confidence Intervals, Journal of Computational and Graphical Statistics, 29:3, 608-619, DOI: 10.1080/10618600.2020.1714633 <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Carpenter, J. and Bithell, J. (2000), Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians. Statist. Med., 19: 1141-1164. <a href="https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9">https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9</a>&lt;1141::AID-SIM479&gt;3.0.CO;2-F <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>PangaeaPanga, This Run Got to ONE LIFE — Clearing 2000 EXPERT Levels (No-Skips) | S2 EP78 <a href="https://youtu.be/hvbGMohca94?t=1070">https://youtu.be/hvbGMohca94?t=1070</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>]]></content>
  </entry>
  <entry>
    <title>How Difficult is 1000 Endless Expert Levels Without Skipping in Super Mario Marker 2?</title>
    <author>
      <name>Keith Yip</name>
      <uri></uri>
    </author>
    <id>https://keithyipkw.github.io/blog/no_skip_smm/</id>
    <updated>2022-03-14T14:20:00Z</updated>
    <published>2022-01-17T13:03:00Z</published>
    <content type="html"><![CDATA[<p>Mario games are fun and challenging. Nintendo has been spending a great deal of effect to design and implement them. In 2015, they released Super Mario Maker. The community could finally legally make and share their levels. With their creativity and dedication, many fun levels which Nintendo would not make were born. Unsurprisingly, the community also made loads of trash levels. Some were only by-products of learning to be great level makers, but the others were purposely built to torment players. In 2019, Nintendo released a sequel to the game Super Mario Marker 2. There has been a new game mode, endless challenge.</p>
<h1 id="no-skip-endless-challenge">No-skip Endless Challenge</h1>
<p>In the endless challenge, you try to complete as many levels the game throws at you as possible. You pick a difficulty and have a certain number of lives to begin. For expert difficulty, you have 15 lives at the start.</p>
<table>
<thead>
<tr>
<th>Difficulty</th>
<th>Number of Starting Lives</th>
</tr>
</thead>
<tbody>
<tr>
<td>Easy</td>
<td>5</td>
</tr>
<tr>
<td>Normal</td>
<td>5</td>
</tr>
<tr>
<td>Expert</td>
<td>15</td>
</tr>
<tr>
<td>Super Expert</td>
<td>30</td>
</tr>
</tbody>
</table>
<p>Each time you die on a level, you lose a life. When you have no remaining lives, the run ends. There are multiple ways to gain lives, but you can only redeem them after clearing a level. If there is a flagpole at the end of a level and you touch the top, you receive a life. Similar to other Mario games, collecting a green mushroom or 100 coins grant you a life too. Besides, you can stomp on multiple enemies without touching the ground or using a shell to kill them consecutively to gain lives as well. You can only redeem as most 3 lives in a level because the game limit that to prevent you to farm lives. A generous level maker will unconditionally give you 3 lives or loads of coins. Conversely, an inexperienced or mean one will give you nothing and even block you from reaching the top of the flagpole to gain a life. In addition to the life gain limit in a level, you can have maximally 99 lives.</p>
<p>The game randomly chooses a level made by the community according to the difficulty as your next level. The exact formula of how the game determines the difficulty of a level is not publically known. It is however strongly correlated to the level clear rate ($ \frac{\text{#clears}}{\text{#trials}} $), player clear rate ($ \frac{\text{#winning players}}{\text{#trying players}} $), and the level maker's number of upload trials. For expert difficulty, you should expect to occasionally encounter a level with a clear rate as low as 2%. For such a level, the community wins 1 out of 50 trials. Very rarely, you will encounter a level with a difficulty like the super expert because the formula does not rate the level accurately enough. It seems to happen more often on new levels which have more uncertainty.</p>
<p>The game allows you to skip levels so you will have a more enjoyable experience in the endless challenge. Your run will not immediately end because you encounter a level poorly made, requiring some skills or knowledge you do not have. However, what hardcore gamers want is the exact opposite. Skipping makes the challenge too easy, so they handicap themselves by forbidding it.</p>
<p>Your life gain on each level needs to be at least on par with your loss on average to keep a run going in the long term. If you are an average player having only a clear rate of a few percent on an expert level, you virtually have no hope of passing 1000 levels, not even 10 levels. Besides a high clear rate, you also need a lower variance in your life change so you can resist a longer streak of bad levels.</p>
<h1 id="theory">Theory</h1>
<p>The most reliable and straightforward way to estimate the difficulty of the no-skip endless challenge is to use an average success rate of many runs. It is the same as estimating the fairness of a coin. We can use the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">binomial proportion</a> to obtain the uncertainty in terms of the <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence interval</a>. A confidence interval is an interval that covers the true parameter with a certain amount of long-run frequency for (hypothetically) repeatedly random sampling. The sampling process works well for poorly performed players whose success rates are close to 0 because their runs do not last long. It is not the case for skilled players. A run may take many hours. For example, with an average of 3 minutes per level, a successful run will take 50 hours to complete. Without many runs, the uncertainty is large, so the overall confidence intervals are too. For example, the 99% confidence interval of a win and a loss is [6.2%, 93.8%]. Therefore, It is worth putting efforts to squeeze out more information from the existing data to shrink the uncertainty. We will model the problem as a Markov process and use the bootstrap to infer the success rate and the confidence intervals. Generally, the estimated success rate should be closer to the true value for all possible true values and samples. Similarly, the overall confidence intervals should be tighter.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/example_life_changes.png"
srcset="/blog/image/no_skip_smm/example_life_changes.png 1x,/blog/image/no_skip_smm/example_life_changes_2x.png 2x"
alt="An example of life changes after beating each of the 1000 levels. The sample success rate is about 50%.">
<figcaption class="text-center text-raven-500">
<p>An example of life changes after beating each of the 1000 levels. The sample success rate is about 50%.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/example_cdf_1.png"
srcset="/blog/image/no_skip_smm/example_cdf_1.png 1x,/blog/image/no_skip_smm/example_cdf_1_2x.png 2x"
alt="The cumulative probability functions calculated by the two methods. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on the distribution of the life changes above. The other one calculated by the binomial proportion is based on one win and one loss.">
<figcaption class="text-center text-raven-500">
<p>The cumulative probability functions calculated by the two methods. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on the distribution of the life changes above. The other one calculated by the binomial proportion is based on one win and one loss.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/example_cdf_2.png"
srcset="/blog/image/no_skip_smm/example_cdf_2.png 1x,/blog/image/no_skip_smm/example_cdf_2_2x.png 2x"
alt="The cumulative probability functions calculated by the two methods with doubling the samples. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on doubling the distribution of the life changes above. The other one calculated by the binomial proportion is based on two wins and two losses.">
<figcaption class="text-center text-raven-500">
<p>The cumulative probability functions calculated by the two methods with doubling the samples. The one calculated by modeling the problem as a Markov process and using the bootstrap is based on doubling the distribution of the life changes above. The other one calculated by the binomial proportion is based on two wins and two losses.</p>
</figcaption>
</figure>

<p>We begin by identifying the essence of the process. At the start of a run, a player has 15 lives. The game randomly draws a level as the next level. After playing a level, the number of life may change with an amount ranging from losing all their lives to gaining 3 lives. If the player loses all their lives, the run ends. If the player loses all their lives, the run ends. Conversely, if they gain so many lives that the total exceeds 99, it will still be 99. The number of life and the change are sufficient to describe the player state and a level, respectively.</p>
<p>Because of the game being closed source, Nintendo not publishing any detail, and insufficient samples, we need to make a few assumptions and simplifications:</p>
<ol>
<li>The game draws a level independent of the number of lives.</li>
<li>The game draws a level independent of the completed levels.</li>
<li>The player's performance is independent of the previous levels.</li>
<li>The player performs practically the same when their number of lives is from 1 to 96 or 97 to 99.</li>
<li>The difficulty of the levels is approximately the same across different runs.</li>
</ol>
<p>There has been no information about the independence in the first assumption. Considering that it seems to be and tight budgets are common in the game industry, it is reasonable to assume that Nintendo chose the simplest implementation. The second assumption is at least a close approximation of the real mechanism. There are 20 million levels<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> as of September 2020. Even if a small subset has the expert difficulty, there should still be enough levels that drawing 1000 levels with replacements is approximately the same as drawing without replacements. Thus, we do not need to worry about which case it is. We can simply model it as drawing with replacements. The remaining assumptions are to reduce the sample requirements. Otherwise, we will need a more sophisticated model or much more samples. Careful analysis may disproof some of the assumptions, but it is out of the scope of this article. Regardless, the following will at least serve as an entry point for more sophisticated analysis even if you do not agree with the assumptions.</p>
<h2 id="modelling-as-a-markov-process">Modelling as a Markov Process</h2>
<p>An easy way of determining the probability is to simulate many runs. Such a simulation can run 500 times per second on my computer. However, because the variance of the runs is very high, the success rate needs 1 million runs to start converging. If all we need is the expected success rate, it is acceptable to let it run for 33 minutes. But we cannot ignore the potentially significant uncertainty in the collected statistics. We can simulate the sampling process and run the 1 million simulations many times, say 1,000, to approximate the uncertainty. That alone will take 23 days of computational time. To ensure that 1,000 simulations are enough, we should repeat it many times too, say 1,000 times, to verify it. The approach quickly becomes impractical if we want to be thorough. Instead, if we model the problem as a Markov process, we can calculate the success rates analytically 2,500 times per second, not once per 33 minutes. Since the solutions are exact, we eliminate the uncertainty in the simulation.</p>
<p>A Markov process is a random process in which the current state determines the future state. In our case, the player's number of lives is the state. By knowing the probability of different life changes after completing a level, we can predict the probabilities of the future number of lives. For example, suppose that a player currently has 10 lives. The probabilities of -1 life, +0 life, and +1 life after finishing the next level are 20%, 30%, and 50%, respectively. The probabilities of having 9, 10, and 11 lives after finishing the next level are 20%, 30%, and 50%, respectively.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/state_change_individual.png"
srcset="/blog/image/no_skip_smm/state_change_individual.png 1x,/blog/image/no_skip_smm/state_change_individual_2x.png 2x"
alt="The probabilities of the future state can be calculated by considering each mututally exclusive branch separately.">
<figcaption class="text-center text-raven-500">
<p>The probabilities of the future state can be calculated by considering each mututally exclusive branch separately.</p>
</figcaption>
</figure>

<p>Furthermore, different numbers of lives are mutually exclusive states, and so do life changes as mutually exclusive events. By the law of total probability, we can separately consider different states then sum the probabilities for all the outcomes. Referring to the example above but now the probabilities of the current state having 9 and 10 lives are 40% and 60%, respectively, the probability of having 9 lives after finishing the next level is</p>

$$
\begin{aligned}
P_\text{next}(\textcolor{red}{♥}=9) ={} & P_\text{current}(\textcolor{red}{♥}=9) \times P(\Delta\textcolor{red}{♥}=0) \\
& + P_\text{current}(\textcolor{red}{♥}=10) \times P(\Delta\textcolor{red}{♥}=-1) \\
={} & 40\% \times 30\% + 60\% \times 20\% \\
={} & 24\%
\end{aligned}
$$

<p>Similarly, the probabilities for 8, 10, and 11 lives are 8%, 38%, and 30%, respectively.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/state_change_combined.png"
srcset="/blog/image/no_skip_smm/state_change_combined.png 1x,/blog/image/no_skip_smm/state_change_combined_2x.png 2x"
alt="The probability of a future state can be calculated separately then summed.">
<figcaption class="text-center text-raven-500">
<p>The probability of a future state can be calculated separately then summed.</p>
</figcaption>
</figure>

<p>It is more convenient to organize the calculation in the following matrix form. In addition, the matrix form allows us to leverage accelerated processings by computers during bootstrapping.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/state_change_matrix.png"
srcset="/blog/image/no_skip_smm/state_change_matrix.png 1x,/blog/image/no_skip_smm/state_change_matrix_2x.png 2x"
alt="The calculation in the matrix form.">
<figcaption class="text-center text-raven-500">
<p>The calculation in the matrix form.</p>
</figcaption>
</figure>

<p>For the actual problem, let</p>
<ul>
<li>$ s_k $ be a 100-dimensional row vector for the states after finishing $ k $ levels,</li>
<li>$ P_s(k,\textcolor{red}{♥}=n) $ be the probability of the state after finishing $ k \in \set{0,1,2,\ldots} $ levels and having $ n \in \set{0,1,2,\ldots,99} $ lives,</li>
<li>$ T $ be a 100×100 right stochastic matrix,</li>
<li>$ P_t(\textcolor{red}{♥}=n,\Delta\textcolor{red}{♥}=m) $ be the probability of transiting from having $ n $ lives to $ (n+m) \in \set{0,1,2,\ldots,99} $ lives.</li>
</ul>

$$
\begin{aligned}
s_k &= \begin{bmatrix}
P_s(k,\textcolor{red}{♥}=0) & P_s(k,\textcolor{red}{♥}=1) & \ldots & P_s(k,\textcolor{red}{♥}=99)
\end{bmatrix} \\ \\

T &= \begin{bmatrix}
1 & 0 & \ldots & 0 \\
P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=-1) & P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=0) & \ldots & P_t(\textcolor{red}{♥}=1,\Delta\textcolor{red}{♥}=98) \\
\vdots & \vdots & \ddots & \vdots \\
P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=-99) & P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=-98) & \ldots & P_t(\textcolor{red}{♥}=99,\Delta\textcolor{red}{♥}=0) \\
\end{bmatrix} \\ \\

s_{k+1} &= s_kT \\

\end{aligned}
$$

<p>The first row in $ T $ is a twist. It allows us to continue to apply the transition to the state even after a run end. This row represents how the <a href="https://en.wikipedia.org/wiki/Absorbing_Markov_chain">absorbing state</a> $ \textcolor{red}{♥}=0 $ is stuck there.</p>
<p>The other rows in $ T $ are mostly some offset versions of the general life change probabilities. Generally, they are the same as the corresponding transition probabilities for each state, i.e., $ P_t(\textcolor{red}{♥}=n,\Delta\textcolor{red}{♥}=m)=P(\Delta\textcolor{red}{♥}=m) $ where $ P(\Delta\textcolor{red}{♥}=m) $ is the probability of life change of $ m \in \set{0,1,2,\ldots,99} $ without considering the minimum (0) and maximum (99) numbers of lives. However, the transition probabilities for 0 lives ought to include the life change probabilities causing the number to drop below 0 and vice versa for 99 lives.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/state_change_transition_row.png"
srcset="/blog/image/no_skip_smm/state_change_transition_row.png 1x,/blog/image/no_skip_smm/state_change_transition_row_2x.png 2x"
alt="In this example, the life change probabilities are for ranging from -3 lives to &#43;1 life. The transition probabilties for the state having 1 life to the state having 0 lives are the sum of the -3, -2 and -1 life probability.">
<figcaption class="text-center text-raven-500">
<p>In this example, the life change probabilities are for ranging from -3 lives to +1 life. The transition probabilties for the state having 1 life to the state having 0 lives are the sum of the -3, -2 and -1 life probability.</p>
</figcaption>
</figure>

<p>Formally, the transition probabilities are</p>

$$
\begin{aligned}

P_t(\textcolor{red}{♥}=n,\Delta\textcolor{red}{♥}=m) = \begin{dcases}
\sum_{q=-\infty}^{m} P(\Delta\textcolor{red}{♥}=q),& \text{if } n+m=0\\
\sum_{q=m}^{\infty} P(\Delta\textcolor{red}{♥}=q),& \text{if } n+m=99\\
P(\Delta\textcolor{red}{♥}=m), & \text{otherwise}
\end{dcases}

\end{aligned}
$$

<p>Starting from the initial state that is 100%, by applying the state change repeatedly, we have the probabilities of the state after finishing any number of levels.</p>

$$
\begin{aligned}

s_k &= s_0T^k
\end{aligned}
$$

<p>We can then calculate the probability of clearing a certain number of levels, and so that of beating the challenge. The probability of beating $ k $ levels is</p>

$$
\begin{aligned}
P(k) &= 1 - P_s(k,\textcolor{red}{♥}=0) \\
&= \sum_{\textcolor{red}{♥}=1}^{99} P_s(k,\textcolor{red}{♥})
\end{aligned}
$$

<h2 id="uncertainty-estimation-by-bootstrapping">Uncertainty Estimation by Bootstrapping</h2>
<p>Sampling the frequencies of different life changes allows us to infer the <a href="https://en.wikipedia.org/wiki/Statistical_population">population</a>, the true probabilities of life changes, and then the parameter, the success rate. It is impossible to know the exact values because it requires the player to play every level infinitely many times to measure the frequencies of different life changes. When we only observe a finite number of samples, there is a random variation between the sample and the population. It is crucial to know the possible variations. Otherwise, we do not know how much the estimation is off. In this article, we will go with the <a href="https://en.wikipedia.org/wiki/Frequentist_inference">frequentist inference</a>. For some simple and common problems, there are formulae to calculate the variations in terms of confidence intervals. However, there seems to be none for this problem. We will use a simulation approach instead.</p>
<p>Bootstrapping is a resampling method based on <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo</a> simulation. The core idea is very simple but yet powerful. We mimic the sampling process of the population by resampling the sample. The sample becomes a known population. With many resamples, our desired <a href="https://en.wikipedia.org/wiki/Statistic">statistic</a>, the sample success rate, forms a distribution that we can compare with the sample. The information allows us to infer the uncertainty in sampling the original population.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_sampling.png"
srcset="/blog/image/no_skip_smm/bootstrap_sampling.png 1x,/blog/image/no_skip_smm/bootstrap_sampling_2x.png 2x"
alt="Usually, we can infer the confidence interval of our interested statistic $ \hat{\theta} $ of our samples $ \hat{X} $ using known formulae.">
<figcaption class="text-center text-raven-500">
<p>Usually, we can infer the confidence interval of our interested statistic $ \hat{\theta} $ of our samples $ \hat{X} $ using known formulae.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_sampling2.png"
srcset="/blog/image/no_skip_smm/bootstrap_sampling2.png 1x,/blog/image/no_skip_smm/bootstrap_sampling2_2x.png 2x"
alt="When there are no known formulae, a simple way to study the uncertainty is to repeat the sampling process many times. It trades the reduction of uncertainty with the knowledge in uncertainty.">
<figcaption class="text-center text-raven-500">
<p>When there are no known formulae, a simple way to study the uncertainty is to repeat the sampling process many times. It trades the reduction of uncertainty with the knowledge in uncertainty.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_resampling.png"
srcset="/blog/image/no_skip_smm/bootstrap_resampling.png 1x,/blog/image/no_skip_smm/bootstrap_resampling_2x.png 2x"
alt="Resampling the only set of observed samples $ \hat{X}^*_1 $ to infer the uncertainty in $ \hat{\theta_1} $ is much cheaper. We can resample $ \hat{X_1} $ many more times than sampling $ X $. It allows us to have the knowledge in uncertainty without sacrificing a smaller uncertainty.">
<figcaption class="text-center text-raven-500">
<p>Resampling the only set of observed samples $ \hat{X}^*_1 $ to infer the uncertainty in $ \hat{\theta_1} $ is much cheaper. We can resample $ \hat{X_1} $ many more times than sampling $ X $. It allows us to have the knowledge in uncertainty without sacrificing a smaller uncertainty.</p>
</figcaption>
</figure>

<p>For simplicity, we will use a non-parameteric bootstrap to estimate some confidence intervals of the success rate. It will directly resample the observed samples. There are many popular non-parameteric bootstraps to choose from, basic (aka empirical/reverse percentile), bootstrap-t (studentized basic), <a href="https://en.wikipedia.org/wiki/Percentile">percentile</a>, <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">bias</a>-corrected (BC), and bias-corrected and accelerated (BCa). They have different assumptions, so they work in different situations<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. The basic bootstrap is based on $ \hat{\theta} - \theta $ and requires it to be <a href="https://en.wikipedia.org/wiki/Pivotal_quantity">pivotal</a>. A pivot is a quantity that is independent of any parameters. It is like a normalized value. The bootstrap-t uses $ \frac{\hat{\theta} - \theta}{\hat{\sigma}} $ instead so it allows variation in the variance. On the contrary, the percentile bootstrap directly works on $ \hat{\theta} $. It only requires existence of a monotonically increasing function $ g $ such that $ \hat{w} = g(\hat{\theta}) - g(\theta) $ is a normal distribution of 0 mean. The amazing and magical part is that you do not need to provide $ g $ at all. The BC bootstrap extends the percentile bootstrap by supporting bias in $ \hat{w} $. Furthermore, the BCa bootstrap supports bias and <a href="https://en.wikipedia.org/wiki/Skewness">skewness</a> in $ \hat{w} $. If you are interested in the theory behind it, I would recommend you to read the section &quot;BOOTSTRAP CONFIDENCE INTERVALS&quot; in <em>Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians</em> by James Carpenter and John Bithell<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>To make non-parametric bootstraps compatible with the way we calculate the transition probabilities, we will simply throw away samples of levels starting with 97 to 99 lives this time. The levels limit the maximum life change. This condition is called <a href="https://en.wikipedia.org/wiki/Censoring_(statistics)">right censoring</a>. For example, consider a level that starts with 97 lives and ends with 99 lives. The life change is +2. When applying the life change to a level that starts with 1 life, we do not know if it should end with 3 or 4 lives. A more advanced way to calculate the transition probabilities is to incorporate all levels into a distribution, but it requires the usage of parametric bootstraps. I will leave it for a future article.</p>
<p>It is unclear if any one of the bootstraps above will work. We will conduct a simluation study of the coverages of the confidence intervals along the way. Using the same resampling technique, we can empirically test the coverages of the confidence intervals. The sample becomes a population, and this time we resample it to simulate the original sampling process. For each resample, we use the bootstraps to calculate the confidence intervals. Because we know the population, and so does the true success rate, we can count the number of instances in which the confidence intervals cover the true success rate. As a result, we can compare the cover rate with the desired rate to determine if the bootstraps work.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_ci.png"
srcset="/blog/image/no_skip_smm/bootstrap_ci.png 1x,/blog/image/no_skip_smm/bootstrap_ci_2x.png 2x"
alt="An illustration of the procedure of the study of the coverages. The sample acts as a population. It is resampled to mimic the original sampling process. A confidence interval is calculated using the bootstrap for each set of resamples. Counting the number of confidence intervals that cover the statistic of the sample gives the actual coverage. It should be within random variations of the nominal coverage.">
<figcaption class="text-center text-raven-500">
<p>An illustration of the procedure of the study of the coverages. The sample acts as a population. It is resampled to mimic the original sampling process. A confidence interval is calculated using the bootstrap for each set of resamples. Counting the number of confidence intervals that cover the statistic of the sample gives the actual coverage. It should be within random variations of the nominal coverage.</p>
</figcaption>
</figure>

<h1 id="data-collection">Data Collection</h1>
<p>We will study the winning probability of one of the most skilled players in Super Mario Marker 2, PangaeaPanga (aka Panga). His playthrough videos are available on his YouTube <a href="https://www.youtube.com/watch?v=2L479zAldzI&amp;list=PL8ooUSoeobwrpfeqSEsRgz0R1PKSjOiJV">playlist</a> will provide us the data. First, we will need to gather the numbers of different life changes. Instead of tediously recording every event manually, we will use computer vision to automate it. We will identify frames containing the number of lives after the completion of each level, recognize the texts, and save them into a list.</p>
<h2 id="frame-detection">Frame Detection</h2>
<p>We start by analyzing the information in the videos. Conveniently, Panga put his number of completed levels on the top right corner. He also showed all the level title scenes, which contained the number of lives. A simple handcrafted detector is more than enough for the job.</p>

<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/title_scene.png"
srcset="/blog/ %!s(<nil>)"
alt="An example of the title scene. The blue and red bounding boxes show the features and the information to extract respectively.">
<figcaption class="text-center text-raven-500">
<p>An example of the title scene. The blue and red bounding boxes show the features and the information to extract respectively.</p>
</figcaption>
</figure>

<p>The title scenes are easily recognizable by a huge yellow box at the top and a black background. We pick some coordinates for the bounding boxes to count the pixels matching our targeted colors. The average colors will not work as our thresholds because the videos are encoded by lossy compression. The colors are distorted and differ at different locations and times. There are no other kinds of frames similar to this, so we can use some generous thresholds without worrying about much false detection of frames. We should be careful that not all the frames are as clean as the example above. Sometimes, there are emotes (icons) flying around and text overlays. We should use some smaller thresholds for the number of pixels.</p>
<p>Using the first detected frame of each title scene to read the number of lives will give a poor result. The text showing the number of lives has a popup animation that enlarges from the bottom left. The text is too small at the beginning of the animation. To work around it, we wait for a larger text by counting the number of white pixels within the bounding box enclosing the widest left digit, 8. The threshold should be small enough to accommodate the thinnest digit, 1.</p>
<p>We will refine the thresholds by examining the failure cases by running the program a couple of times after implementing the text recognition in the next step. Below is a peek of the final thresholds:</p>
<style>
.color_block_shadow {
    text-shadow:-0.5px 0.5px 2px #00000080;
}
</style>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Color</th>
<th>Lower Threshold</th>
<th>Upper Threshold</th>
<th>Area</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tittle banner</td>
<td><span class="color_block_shadow" style="color:#FBCB03">■</span> #FBCB03</td>
<td><span class="color_block_shadow" style="color:#E6BE00">■</span> #E6BE00</td>
<td><span class="color_block_shadow" style="color:#FFDC14">■</span> #FFDC14</td>
<td>&gt;70%</td>
</tr>
<tr>
<td>Background</td>
<td><span class="color_block_shadow" style="color:#000000">■</span> #000000</td>
<td><span class="color_block_shadow" style="color:#000000">■</span> #000000</td>
<td><span class="color_block_shadow" style="color:#060606">■</span> #060606</td>
<td>&gt;70%</td>
</tr>
<tr>
<td>Stable life text</td>
<td><span class="color_block_shadow" style="color:#FFFFFF; text-shadow:-0.5px 0.5px 3px #00000080">■</span> #FFFFFF</td>
<td><span class="color_block_shadow" style="color:#C8C8C8;">■</span> #C8C8C8</td>
<td><span class="color_block_shadow" style="color:#FFFFFF;">■</span> #FFFFFF</td>
<td>&gt;14%</td>
</tr>
</tbody>
</table>
<p>The source code is available in the <a href="https://github.com/keithyipkw/blog/tree/master/post_supplements%5Cno_skip_smm%22">GitHub repository</a> of this site. I will not show it and explain the implementation here. There are plenty of tutorials on fundamental computer vision on the Internet.</p>
<h2 id="optical-character-recognition">Optical Character Recognition</h2>
<p>After locating the frames, the next step is to recognize the text corresponding to the number of lives and completed levels. There are many ways to do it. Simple template matching provided by OpenCV should work, but we will use <a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a> instead. Tesseract comes with pre-trained models, so we do not need to prepare any templates to match or train. Unfortunately, whitelisting the built-in pre-trained English model with digits just happens to perform poorly. It is probably because the architecture of the neural network does not handle whitelists well. Before trying to train a specialized model using the numbers from the frames or switching to template matching, we should try other pre-trained models first. A Tesseract contributor, Shreeshrii, shared their refined models in his <a href="https://github.com/Shreeshrii/tessdata_shreetest">GitHub repository</a>. <code>digits.traineddata</code> works much better than the built-in English model.</p>
<h2 id="data-cleansing">Data Cleansing</h2>
<p>After implementing the process, we get the level information that requires some data cleansing. We should not blindly accept the output as there may be errors in the output. Panga gave up several times which he had rough starts. Besides, there may be recognition errors. An easy way to spot major errors is to check for invalid level changes and life changes. Generally, each consecutive level should have +1 level change and less than or equal to +3 life change. For smaller incorrect life changes, the only way to eliminate them is to manually compare the output against the videos. We will skip it and assume that they are insignificant to the estimated success rate.</p>
<p>There are only 12 erroneous levels out of 1058 levels. The error rate is 1.13%. At this point, it does not worth the time to improve the frame detector and text recognition. We should manually fix those errors.</p>
<h1 id="simulation">Simulation</h1>
<p>Before proceeding to the calculation, we need to filter out levels starting with 97 to 99 lives. There are 726 remaining levels that we can use. He gained lives 52.5% of the time, lost lives 26.6% of the time, and neither 20.9% of the time. On average, he gained 0.233 lives after playing a level. The distribution is as follow:</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/pangas_life_changes.png"
srcset="/blog/image/no_skip_smm/pangas_life_changes.png 1x,/blog/image/no_skip_smm/pangas_life_changes_2x.png 2x"
alt="Panga&#39;s performance on the 726 levels.">
<figcaption class="text-center text-raven-500">
<p>Panga's performance on the 726 levels.</p>
</figcaption>
</figure>

<h2 id="sample-success-rate">Sample Success Rate</h2>
<p>Now we can calculate Panga's sample success rate of beating the challenge by applying the theory of Markov process. The calculation is also a building block in the bootstrap for calculating the confidence interval. The sample probability is 61.6%.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/pangas_sample_success_rates.png"
srcset="/blog/image/no_skip_smm/pangas_sample_success_rates.png 1x,/blog/image/no_skip_smm/pangas_sample_success_rates_2x.png 2x"
alt="The estimated probability of Panga beating various numbers of levels in the endless challenge without skipping.">
<figcaption class="text-center text-raven-500">
<p>The estimated probability of Panga beating various numbers of levels in the endless challenge without skipping.</p>
</figcaption>
</figure>

<h2 id="bootstrap">Bootstrap</h2>
<p>We will use the bootstrap module in <a href="https://github.com/bashtage/arch">Arch</a> to calculate the confidence interval. I choose it instead of SciPy because it is easier to experiment with. It provides more bootstrap methods and natively supports resuing the results when changing bootstrap methods.</p>
<h3 id="simulation-study-of-coverages">Simulation Study of Coverages</h3>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_ci.png"
srcset="/blog/image/no_skip_smm/bootstrap_ci.png 1x,/blog/image/no_skip_smm/bootstrap_ci_2x.png 2x"
alt="Recall of the procedure of testing the coverage.">
<figcaption class="text-center text-raven-500">
<p>Recall of the procedure of testing the coverage.</p>
</figcaption>
</figure>

<p>If you use Arch directly, you will encounter the following exception</p>
<blockquote>
<p>Empirical probability used in bias correction is 0 or 1, and so bias cannot be corrected. This may occur in extremum statistics that are not well approximated by a normal in a finite sample.</p>
</blockquote>
<p>For us, it happens when a simulated sample $ \hat{X}_r^* $ excludes all occurrences of -15 lives and below. The maximum life loss is 14, so the survival probability of the first level is 100% for all the bootstrap resamples $ \hat{X}_{r,s}^* $. Patching Arch to handle the problem is fairly easy and fast.</p>
<p>We start by preliminarily checking the coverages of 95% confidence interval for several common bootstrap methods, basic, percentile, bias-corrected (BC), and bias-corrected and accelerated (BCa). 1000 bootstrap resamples repeating 1000 times is enough. It means that there are 1000 simulated samples $ \hat{X}_r^* $ and 1000 bootstrap resamples $ \hat{X}_{r,s}^* $ for each simulated samples. After obtaining a confidence interval for each of the 1000 true success rate $ \hat{\theta} $, we count numbers of confidence intervals that contain their corresponding success rate. I ran the test for levels from 1 to 3000 and plotted the graph below.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_ci_all_0.95.png"
srcset="/blog/image/no_skip_smm/bootstrap_ci_all_0.95.png 1x,/blog/image/no_skip_smm/bootstrap_ci_all_0.95_2x.png 2x"
alt="The coverages of the 95% confidence intervals for 1000 bootstrap resamples repeating 1000 times.">
<figcaption class="text-center text-raven-500">
<p>The coverages of the 95% confidence intervals for 1000 bootstrap resamples repeating 1000 times.</p>
</figcaption>
</figure>

<p>The dotted lines in the graph show the thresholds rejecting the null hypotheses that the corresponding measured coverages were the same as the nominal coverage of 95%. It was clear that the basic bootstrap gave different coverages. On the contrary, we could not reject the null hypotheses for the percentile, the BC, and the BCa bootstrap. You should be cautious that it did not mean that the coverages were the same as the nominal coverage. It is impossible to prove these null hypotheses. To tell if those methods gave the &quot;same&quot; coverages as the nominal coverage, we should perform <a href="https://en.wikipedia.org/wiki/Equivalence_test">equivalence tests</a>. From now on, we will exclude the basic bootstrap and increase the number of resamples and repeats.</p>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_ci_0.95.png"
srcset="/blog/image/no_skip_smm/bootstrap_ci_0.95.png 1x,/blog/image/no_skip_smm/bootstrap_ci_0.95_2x.png 2x"
alt="The coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.">
<figcaption class="text-center text-raven-500">
<p>The coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</p>
</figcaption>
</figure>

<figure>
<table>
<thead>
<tr>
<th>Levels</th>
<th>Percentile</th>
<th>BC</th>
<th>BCa</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>94.9%</td>
<td>94.9%</td>
<td>95.0%</td>
</tr>
<tr>
<td>2000</td>
<td>94.8%</td>
<td>94.8%</td>
<td>95.2%</td>
</tr>
<tr>
<td>3000</td>
<td>94.8%</td>
<td>94.9%</td>
<td>95.1%</td>
</tr>
</tbody>
</table>

<figcaption>The coverages of the 95% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</figcaption>
</figure>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/bootstrap_ci_0.99.png"
srcset="/blog/image/no_skip_smm/bootstrap_ci_0.99.png 1x,/blog/image/no_skip_smm/bootstrap_ci_0.99_2x.png 2x"
alt="The coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.">
<figcaption class="text-center text-raven-500">
<p>The coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</p>
</figcaption>
</figure>

<figure>
<table>
<thead>
<tr>
<th>Levels</th>
<th>Percentile</th>
<th>BC</th>
<th>BCa</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>99.0%</td>
<td>99.0%</td>
<td>99.2%</td>
</tr>
<tr>
<td>2000</td>
<td>99.0%</td>
<td>99.0%</td>
<td>99.2%</td>
</tr>
<tr>
<td>3000</td>
<td>98.9%</td>
<td>99.0%</td>
<td>99.2%</td>
</tr>
</tbody>
</table>

<figcaption>The coverages of the 99% confidence intervals for 40000 bootstrap resamples repeating 4000 times.</figcaption>
</figure>
<p>They all performed similarly. I will not perform the equivalence tests here. Let us assume that they are equivalent to our use. We will use the BCa for the analysis.</p>
<h1 id="result">Result</h1>
<p>Combining the sample probability with the bootstrap result using the BCa for 40000 resamples gave the following result. As a bonus, I calculated all the probabilities for levels up to 3000. You should keep in mind that the probabilities of the confidence intervals shown below refer to the long-run frequencies of covering the true success rates for (hypothetical) repeated random sampling. A particular confidence interval either covers or does not cover the true success rate. After getting a sample and revealing the confidence interval, the cover rate of the confidence interval is only epistemic (one's ignorance). Further interpretations require a deeper understanding of the properties of bootstrapping. The definition of confidence intervals is loose in which it only concerns the cover rate. Some procedures give opposite widths or counter-intuitive intervals. Richard D. Morey, Rink Hoekstra, Jeffrey N. Rouder, Michael D. Lee, and Eric-Jan Wagenmakers explained the issues nicely in <em>The fallacy of placing confidence in confidence intervals</em><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Below is the result:</p>
<table>
<thead>
<tr>
<th>Levels</th>
<th>Probability</th>
<th>95% CI</th>
<th>99% CI</th>
</tr>
</thead>
<tbody>
<tr>
<td>1000</td>
<td>61.6%</td>
<td>[15.7%, 85.4%]</td>
<td>[5.84%, 89.4%]</td>
</tr>
<tr>
<td>2000</td>
<td>58.9%</td>
<td>[8.38%, 86%]</td>
<td>[1.78%, 90.2%]</td>
</tr>
<tr>
<td>3000</td>
<td>56.3%</td>
<td>[3.96%, 86%]</td>
<td>[0.43%, 90.1%]</td>
</tr>
</tbody>
</table>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/pangas_probabilities.png"
srcset="/blog/image/no_skip_smm/pangas_probabilities.png 1x,/blog/image/no_skip_smm/pangas_probabilities_2x.png 2x"
alt="Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.">
<figcaption class="text-center text-raven-500">
<p>Probabilities of Panga successfully beating various numbers of levels. The darkest color denotes the medians of the probabilities.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/pangas_probabilities_simplified.png"
srcset="/blog/image/no_skip_smm/pangas_probabilities_simplified.png 1x,/blog/image/no_skip_smm/pangas_probabilities_simplified_2x.png 2x"
alt="Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.">
<figcaption class="text-center text-raven-500">
<p>Probabilities of Panga successfully beating various numbers of levels. Only 95% and 99% confidence intervals are shown.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/pangas_probability_1000.png"
srcset="/blog/image/no_skip_smm/pangas_probability_1000.png 1x,/blog/image/no_skip_smm/pangas_probability_1000_2x.png 2x"
alt="Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.">
<figcaption class="text-center text-raven-500">
<p>Probability of Panga successfully beating 1000 levels. It is a vertical slice of the level 1000 of the previous two graphs, and like a graph of the cumulative probability but with the 0% - 50% part being flipped upward.</p>
</figcaption>
</figure>



<figure>
<img
class="mx-auto leading-none"
src="/blog/image/no_skip_smm/pangas_cdf_1000.png"
srcset="/blog/image/no_skip_smm/pangas_cdf_1000.png 1x,/blog/image/no_skip_smm/pangas_cdf_1000_2x.png 2x"
alt="Cumulative probability of Panga successfully beating 1000 levels.">
<figcaption class="text-center text-raven-500">
<p>Cumulative probability of Panga successfully beating 1000 levels.</p>
</figcaption>
</figure>

<h1 id="conclusion">Conclusion</h1>
<p>Undoubtedly, beating 1000 endless expert levels without skipping in Super Mario Marker 2 is hard for most players. It is not for ones without top-notch skills and strong determination. This article has quantified the difficulty by using the theory of the Markov process, bootstrapping, and one of the best players' data. You are likely to have even more questions about this challenge other than the success rate. In the future, we will delve deeper into the details in <a href="https://keithyipkw.github.io/blog/no_skip_smm_part_2/">part 2</a>.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>A tweet by Nintendo of America on 4 Sep, 2020. <a href="https://twitter.com/NintendoAmerica/status/1301928016004644864">https://twitter.com/NintendoAmerica/status/1301928016004644864</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Carpenter, J. and Bithell, J. (2000), Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians. Statist. Med., 19: 1141-1164. <a href="https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9">https://doi.org/10.1002/(SICI)1097-0258(20000515)19:9</a>&lt;1141::AID-SIM479&gt;3.0.CO;2-F <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Morey, R.D., Hoekstra, R., Rouder, J.N. et al. The fallacy of placing confidence in confidence intervals. Psychon Bull Rev 23, 103–123 (2016). <a href="https://doi.org/10.3758/s13423-015-0947-8">https://doi.org/10.3758/s13423-015-0947-8</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>]]></content>
  </entry>
  <entry>
    <title>Error Bars are Deceptive</title>
    <author>
      <name>Keith Yip</name>
      <uri></uri>
    </author>
    <id>https://keithyipkw.github.io/blog/error-bars-are-deceptive/</id>
    <updated>2021-08-28T14:07:00Z</updated>
    <published>2021-08-28T14:07:00Z</published>
    <content type="html"><![CDATA[<p>I will start by asking two questions. In addition to your answers and reasons, you may also imagine how the general public and your audience will respond.</p>
<h1 id="quiz">Quiz</h1>
<h2 id="question-1">Question 1</h2>
<p>In a factory manufacturing rods, the measurements of the rods are shown in the following graph:</p>

<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q1.png"
srcset="/blog/ %!s(<nil>)"
alt="Measurements of the rods.">
<figcaption class="text-center text-raven-500">
<p>Measurements of the rods.</p>
</figcaption>
</figure>

<p>What is the quality?</p>
<ol>
<li>The quality is met.</li>
<li>The quality is not met.</li>
<li>There is insufficient information to tell.</li>
</ol>
<h2 id="question-2">Question 2</h2>
<p>In a study of comparing two drugs, the effects are shown in the following graph:</p>

<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q2.png"
srcset="/blog/ %!s(<nil>)"
alt="The effects of the drugs.">
<figcaption class="text-center text-raven-500">
<p>The effects of the drugs.</p>
</figcaption>
</figure>

<p>Which of the following is correct?</p>
<ol>
<li>The effect of drug B is significantly (p ≤ 0.05) different from drug A.</li>
<li>The result is insignificant.</li>
<li>It is too close to tell by looking.</li>
<li>There is insufficient information to tell.</li>
</ol>
<h2 id="answers">Answers</h2>
<details>
    <summary>Click this to reveal the answsers.</summary>
    <br>
<p>Both the answers are &quot;there is insufficient information to tell.&quot; Congratulation if you get them right. You have high enough graph literacy to dodge my traps. Do not worry if you do not get them. I will explain in detail below.</p>
<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q1_extreme.png"
srcset="/blog/ %!s(<nil>)"
alt="This is an extreme case of the measurements of the rods. The original error bar only shows ±1 standard deviation. There are 80% of the rods exceeding the tolerance.">
<figcaption class="text-center text-raven-500">
<p>This is an extreme case of the measurements of the rods. The original error bar only shows ±1 standard deviation. There are 80% of the rods exceeding the tolerance.</p>
</figcaption>
</figure>

</details>  
<h1 id="introduction">Introduction</h1>
<p>Randomness is an unavoidable part of the physical world. It is incomplete to talk about a <a href="https://en.wikipedia.org/wiki/Physical_quantity">physical quantity</a> without its associated error. The pair is typical written in the form of &quot;value ± error&quot; for symmetric errors. In the form of a graph, it is popular to present an error with a bar. An error bar is a straight line with two caps at the end, signaling the size of the error. However, there are some intrinsic and extrinsic problems causing it to be an ineffective representation. Error bars hide important information, namely coverages and distributions. They frequently cover less than the ranges of the errors. Despite that they emphasize the values within, the values outside are usually still important. They are visually uniform, but the underlying distributions are not. Their annotations are typically outside their graphs, somewhere in the text.</p>
<p style="background-color: whitesmoke; padding: 0.75rem; font-size: 1rem">All the source code plotting the graphs is available <a href="https://github.com/keithyipkw/blog/tree/master/post_supplements\error-bars">here</a>.</p>
<h1 id="meaning-of-error-bars">Meaning of Error Bars</h1>
<p>Error bars are graphical representations of errors. There are different kinds of errors, e.g., sampling errors, estimation errors, manufacturing errors, etc. Below is a summary of common quantifications of them.</p>
<table>
<thead>
<tr>
<th>Depictions</th>
<th>Common Coverages</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://en.wikipedia.org/wiki/Standard_deviation">Standard Deviation</a></td>
<td>±1 standard deviation</td>
</tr>
<tr>
<td><a href="https://en.wikipedia.org/wiki/Standard_error">Standard Error</a></td>
<td>±1 standard deviation</td>
</tr>
<tr>
<td><a href="https://en.wikipedia.org/wiki/Confidence_interval">Confidence Interval</a><br/>or <a href="https://en.wikipedia.org/wiki/Credible_interval">Credible Interval</a></td>
<td>95%<br/>99.7%<br/>99%</td>
</tr>
<tr>
<td><a href="https://en.wikipedia.org/wiki/Quantile">Quantile</a></td>
<td>none</td>
</tr>
</tbody>
</table>
<h1 id="information-hiding">Information Hiding</h1>
<p>An error distribution can be in any shape. The middle of an error bar is only a line. It does not carry any information about the distribution. Even worst, it looks like a uniform distribution. It is only acceptable when all the following strict conditions are met:</p>
<ol>
<li>Everyone agrees with the boundaries.</li>
<li>The bars show full ranges of errors.</li>
<li>Only the boundaries matter.</li>
</ol>
<p>Not agreeing with the boundaries does not render a result useless. Showing the distribution allows the audience to apply their criteria. In addition, a larger portion being further away from boundaries is practically better. There may be errors unaccounted for or future errors. Similarly, error bars are unhelpful when there are overlappings. Even if the overlapped ranges of different distributions are the same, the overlapped probabilities are generally different. When the audience does not have a certain degree of prior knowledge, they will use some guesswork to fill in the blank.</p>
<h1 id="inconsistent-coverages">Inconsistent Coverages</h1>
<p>The smaller the errors are, the better. Making them look smaller than they should is very tempting. Drawing only ±1 standard deviation is a common trick. At first glance, an error bar seems to suggest the audience to concern only about its range within. This coverage is rarely enough for real-world applications. For example, an error bar referring to ±1 standard deviation only covers 68% of samples for a normal distribution. The audience needs to extend the bars themselves, likely in their minds, if they know that they should. It may be easy to do so with one or two error bars but extremely challenging for multiple error bars or multi-way comparisons.</p>
<h1 id="alternatives">Alternatives</h1>
<p>Violin plots are good alternatives. Their effectiveness was proven in the paper &quot;Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error&quot;<sup><a href="#1">[1]</a></sup>. Violin plots use areas to present distributions. They leverage basic humans' visual skills in counting. As a result, the audience can judge the errors with a glance in simple cases. If error bars are drawn, they should be extended until meeting the criteria.</p>

<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q1_violin.png"
srcset="/blog/ %!s(<nil>)"
alt="The graph in question 1 but being a normal distribution and was drawn as a violin plot.">
<figcaption class="text-center text-raven-500">
<p>The graph in question 1 but being a normal distribution and was drawn as a violin plot.</p>
</figcaption>
</figure>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q2_violin.png"
srcset="/blog/ %!s(<nil>)"
alt="The graph in question 2 but was drawn as a violin plot. The widths are normalized to their corresponding probability mass functions, so the maximum widths are the same.">
<figcaption class="text-center text-raven-500">
<p>The graph in question 2 but was drawn as a violin plot. The widths are normalized to their corresponding probability mass functions, so the maximum widths are the same.</p>
</figcaption>
</figure>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q2_violin_norm_area.png"
srcset="/blog/ %!s(<nil>)"
alt="The graph in question 2 but was drawn as a violin plot. The widths are normalized to their corresponding cumulative distribution functions, so the areas are the same.">
<figcaption class="text-center text-raven-500">
<p>The graph in question 2 but was drawn as a violin plot. The widths are normalized to their corresponding cumulative distribution functions, so the areas are the same.</p>
</figcaption>
</figure>

<p>Gradient plots are other proven alternatives<sup><a href="#1">[1]</a></sup>. The audience understands that they represent fuzzy values without much explanation. The drawback is that it is difficult to draw and present them properly. Color is a complicated subject and hard to be done right. The graphs should accurately use colors to represent values. Otherwise, the audience will overweigh the values with unintentionally increased contrasts or vice versa. Typical software operates in non-linear perceptual color spaces. You are likely to draw perceptually distorted gradients. Besides, gradients in different colors give different visual contrasts. It is a problem if you use multiple colors. Furthermore, the audience may view the graphs with reduced contrasts or color distortions caused by poor conditions. They may view the graphs on incorrectly color-managed prints, using uncalibrated monitors, using projectors with environment light on, or viewing at large angles. There are likely 0.2% - 9% of your audience having color vision deficiency <sup><a href="#2">[2]</a></sup>. They may see reduced contrasts too. How much depends on how you draw your graphs. There are too many things to go wrong with gradient plots. Violin plots are clearer, simpler, and safer. It does not mean that gradient plots are inferior. There are trade-offs. It is just unfavorable to gradient plots. There exist situations where color accuracy is less of a problem, e.g., when the lengths of the gradients are short in a zoomed-out scale. In such cases, gradient plots may be good choices.</p>

<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q1_gradient.png"
srcset="/blog/ %!s(<nil>)"
alt="The graph in quesiton 1 but being a normal distribution and was drawn as a gradient plot. The gradient is drawn in a straight-forward way without careful color handling.">
<figcaption class="text-center text-raven-500">
<p>The graph in quesiton 1 but being a normal distribution and was drawn as a gradient plot. The gradient is drawn in a straight-forward way without careful color handling.</p>
</figcaption>
</figure>


<figure>
<img
class="mx-auto leading-none"
src="/blog/image/error_bar/q2_gradient.png"
srcset="/blog/ %!s(<nil>)"
alt="The graph in question 2 but was drawn as a gradient plot. The gradients are drawn in a straight-forward way without careful color handling. With a properly calibrated monitor, you should see that the orange bar disappears near 1/2 to 2/3 of the last standard deviations counting from the center.">
<figcaption class="text-center text-raven-500">
<p>The graph in question 2 but was drawn as a gradient plot. The gradients are drawn in a straight-forward way without careful color handling. With a properly calibrated monitor, you should see that the orange bar disappears near 1/2 to 2/3 of the last standard deviations counting from the center.</p>
</figcaption>
</figure>

<h1 id="conclusion">Conclusion</h1>
<p>Error bars are inadequate to display errors. They hide information and are frequently drawn short. In summary, they are deceptive. To communicate with your audience more effectively, I would suggest you adopt the following measures:</p>
<ol>
<li>Use violin plots.</li>
<li>Draw the full coverages of the error criteria.</li>
<li>Annotate the error bars in the graphs.</li>
</ol>
<p>There are situations where gradient plots shine. However, gradient plots are tricky to draw and show right. You should consider the trade-offs carefully.</p>
<hr>
<p><a id="1">[1]</a>
M. Correll and M. Gleicher, &quot;Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error,&quot; in IEEE Transactions on Visualization and Computer Graphics, vol. 20, no. 12, pp. 2142-2151, 31 Dec. 2014, doi: 10.1109/TVCG.2014.2346298.</p>
<p><a id="2">[2]</a> Birch J. Worldwide prevalence of red-green color deficiency. J Opt Soc Am A Opt Image Sci Vis. 2012;29(3):313-320. doi:10.1364/JOSAA.29.000313</p>]]></content>
  </entry>
  <entry>
    <title>A Case Study of Taking Back Digital Privacy</title>
    <author>
      <name>Keith Yip</name>
      <uri></uri>
    </author>
    <id>https://keithyipkw.github.io/blog/taking-back-digital-privacy/</id>
    <updated>2021-06-27T11:36:00Z</updated>
    <published>2021-06-27T11:36:00Z</published>
    <content type="html"><![CDATA[<p>This article is a case of helping my friend taking back their digital privacy. They are non-technical and lived together with their adversary. This article may help you if you are in a similar situation. However, you should seriously consider the possibility that the possessive persons would physically harm others when they are denied.</p>
<h1 id="background">Background</h1>
<p>Alex lived with Billy. Alex was quite a technical person but Billy was totally not. It was natural for Billy to rely on Alex on all technological problems. Alex set up all Billy's digital accounts and smartphones. It seemed to be fine until Alex's possession over Billy surfaced. Alex spied on Billy's emails, Whatsapp, and iCloud photos.</p>
<p>Billy decided to take back their privacy. Billy and their friend, friend C, started to do so but encountered a roadblock. They consulted me. During the discussion, I discovered that their original plan would not work. Alex could regain control in no time. It was actually not straight forward for Billy.</p>
<h1 id="situation">Situation</h1>
<p>The challenges were</p>
<ol>
<li>Alex had total control over Billy's accounts.</li>
<li>Alex and Billy lived together.</li>
<li>Alex had a tremendous advantage of technological knowledge over Billy.</li>
<li>Billy needed to write down all account passwords in a notebook to remember them.</li>
<li>Billy generally required face-to-face assistance on computer related issues but was overseas and far away from friend C and me.</li>
<li>Billy did not have a trustable computer which was confidently free of spywares.</li>
</ol>
<p>After a more in-depth discussion, I confirmed a set of fulfilled prerequisites just enough to take back the privacy</p>
<ol>
<li>Billy had been locking the phone with a password only.</li>
<li>They were able to physically secure the phone.</li>
<li>They had a nearby helping friend, friend D, who could follow my instructions with only remote assistance.</li>
<li>friend D had a PC.</li>
</ol>
<h1 id="outline-of-the-plan">Outline of the Plan</h1>
<p>A written plan was necessary. The process involved many steps which were easy to miss. Some were even needed to be verified first. The process would be in a hurry too. Taking back the accounts would trigger the service providers to send emails to the registered email address. It was nice to have a PC and a stable and high-speed internet connection so we could act efficiently but the process would still take several hours. If Alex checked Billy's emails during the process, they could simply change the passwords to block us.</p>
<p>The outline of the process was</p>
<ol>
<li>to make a backup.</li>
<li>to take back as many accounts as possible.</li>
<li>to migrate the forever insecure accounts.</li>
<li>to strengthen up the security setting of the phone.</li>
<li>to educate Billy on privacy protection.</li>
<li>to do routine security examinations.</li>
</ol>
<h2 id="making-backups">Making Backups</h2>
<p>Billy used an iPhone. The only backup was on iCloud. It was unsafe to rely on using iCloud to back up while changing it. Using a PC to back up the phone gave an isolated copy.</p>
<p>You do not need to delete the backup immediately if you trust your helpers or computers. You may wait for a month to see if there is any missing data caused by the migration.</p>
<h2 id="taking-back-the-accounts">Taking Back the Accounts</h2>
<p>It involved removing signed in devices, changing the passwords, the recovery settings, and strengthening up the security settings. Changing the recovery and security settings would prevent Alex from taking control of the accounts again. I chose SMS as the 2-factor authentication here. I generally discourage anyone to do so because SMS was barely better than nothing. You should use an authenticator. Consider the complications for Billy to adopt them, especially in this stressful situation, it was acceptable to settle on SMS for the time being.</p>
<p>If you do not know the passwords, you may try to recover them from your computers. They may be stored in password managers in your browsers or OSes. If you do not have all your passwords, it worths trying existing passwords from other accounts.</p>
<h2 id="migration-of-insecure-accounts">Migration of Insecure Accounts</h2>
<p>Replacement accounts would be needed for the accounts if they could not be taken control of. It would need data migration. That of iCloud was the most dangerous. One wrong step would wipe the data. This was the reason to have an isolated backup.</p>
<h2 id="strengthening-up-the-security-setting-of-the-phone">Strengthening up the Security Setting of the Phone</h2>
<p>The phone, having access and the ability to grant new access to the accounts, would be an attractive target for Alex to attack. Existing iCloud backup and jailbreaks should be removed. All biometric authentications, e.g. face unlock and fingerprint unlock, should be disabled. I confirmed there was no iCloud locks on the phone so there was no need to buy a new phone. I also confirmed that Billy was the solo legal owner of the phone number. Alex could not get a new SIM card for the phone number from the telecom provider without breaking some serious law. The SIM card was removable so a SIM lock was required to prevent Alex from simply loading Billy's SIM into another phone. SMS previews on the lock screen should be disabled to prevent Alex from peeking the codes for 2-factor authentications.</p>
<h2 id="user-education-on-privacy-protection">User Education on Privacy Protection</h2>
<p>All the defenses above would be broken if Billy was not careful. Billy should keep the phone nearby all the time. They should never give Alex the phone. Billy might be used to seek help from Alex but this should be stopped. They should never log in to these accounts in the shared PC too.</p>
<h2 id="periodic-follow-up-examinations">Periodic Follow-up Examinations</h2>
<p>To ensure the effectiveness of the measures, periodic follow-up examinations would be necessary. There might be loopholes in the plan, flaws in the execution, or slip-ups in securing the phone. Luckily, major service providers showed users all record of login activities nowadays. It would be pretty easy to check if Alex broke into Billy's accounts.</p>
<h1 id="planned-procedure">Planned Procedure</h1>
<p>I will not show any screenshots here. UI changes frequently. It is very easy to Google the screenshots if you get lost. You will need to modify it to fit your needs as well.</p>
<h2 id="disabling-mobile-network">Disabling Mobile Network</h2>
<ol>
<li>Swipe from the bottom of the screen up to show the control center.</li>
<li>Tap the (green) tower icon.</li>
</ol>
<h2 id="itunes-backup">iTunes Backup</h2>
<ol>
<li>Connect the phone to the PC.</li>
<li>Click the small phone icon in iTunes.</li>
<li>Confirm if there is no sensitive data on the phone.</li>
<li>If there is sensitive data, enable password-protected backup in iTunes. Remember to disable it later.</li>
<li>Click the backup button to backup the whole phone.</li>
</ol>
<h2 id="downloading-original-photos-and-videos">Downloading Original Photos and Videos</h2>
<ol>
<li>Check data usage of iCloud photos by going to Settings → Apple ID → iCloud → Manage Storage.</li>
<li>Check local free disk space and data usage of the Photos app by going to Settings → iPhone Storage.</li>
<li>Proceed if the free disk space plus the data usage of the Photos app is more than the data usage of iCloud photos.</li>
<li>Download the photos by going to Settings → Photos → select Download and Keep Originals.</li>
<li>Open Photos → the Photos tab.</li>
<li>Wait for the download to complete as shown at the bottom.</li>
</ol>
<h2 id="itunes-backup-again">iTunes Backup Again</h2>
<ol>
<li>Connect the phone to the PC.</li>
<li>Click the small phone icon in iTunes.</li>
<li>Click the &quot;Backup&quot; button to backup the whole phone.</li>
</ol>
<h2 id="separate-photos-and-videos-backup">Separate Photos and Videos Backup</h2>
<p>Live photos and burst shots are not copied by this method.</p>
<ol>
<li>Unplug then plug the phone into a PC.</li>
<li>Select &quot;Trust&quot; in the &quot;Trust This Computer&quot; prompt on the phone.</li>
<li>Open the phone like a USB stick in the file explorer on the PC.</li>
<li>Copy the enclosing folder to the PC.</li>
<li>Check the size of the copied folder.</li>
<li>At least open some photos to check the integrity.</li>
</ol>
<h2 id="taking-back-accounts">Taking Back Accounts</h2>
<p>The basic ideas are to sign out all other devices, reset all recovery methods and passwords, and enable 2-factor authentications. It should be started from the email that was used to registered other services. The following steps work for Google accounts. Those for other major service providers like Microsoft are similar. There should be no new logins on the phone until signing out all iCloud logins on other devices. Otherwise, iCloud backup can be abused to continue to spy on you by restoring the backup on other devices. If you cannot do any of the steps, you will need to create a new one for the corresponding account.</p>
<ol>
<li>Login in a browser on a PC.</li>
<li>Go to Account Settings → Security → Your Devices → Manage devices.</li>
<li>Sign out all other devices.</li>
<li>Under &quot;Ways that we can verify that it's you&quot;, remove recovery emails and phones which are under other people's control. These steps apply to all recovery accounts and all recovery accounts of them.</li>
<li>Change all other recovery methods, e.g. security questions, if there are any.</li>
<li>Change the password.</li>
<li>Enable 2-Step Verification with SMS.</li>
</ol>
<h3 id="sign-out-of-other-whatsapp-logins">Sign out of Other Whatsapp Logins</h3>
<ol>
<li>Open Whatsapp.</li>
<li>Open Settings.</li>
<li>Tap WhatsApp Web/Desktop.</li>
<li>Remove all devices listed there. Being asked to scan a QR code means that there are currently no other logins.</li>
</ol>
<h2 id="spyware-removal">Spyware Removal</h2>
<ol>
<li>Open Settings.</li>
<li>Scroll down to tap Privacy.</li>
<li>Tap Location Services.</li>
<li>Check for unknown Apps.</li>
<li>Tap back.</li>
<li>Check for unknown Apps under Camera and Microphone too.</li>
<li>Go back to the home screen.</li>
<li>Check for unknown Apps one by one.</li>
<li>Delete any unknown and preferably unused Apps.</li>
<li>If you remove any apps, do an iTunes backup again.</li>
</ol>
<h2 id="jailbreak-removal">Jailbreak Removal</h2>
<p>I am not a jailbreak expert. I am not sure if it is possible to hide jailbreak status. I could not find any resources online. My guess is that hiding jailbreak status should be possible when ones have root access. There are apps to hide some other apps on the home screen (SpringBoard). If you want to be sure, it is better to do a DFU restore anyway.</p>
<ol>
<li>Check if there is an app called Cydia. Continue if yes.</li>
<li>Sign out iCloud.</li>
<li>Backup using the PC again.</li>
<li>Do a DFU restore.</li>
<li>Restore the phone from the PC.</li>
<li>Do an iTunes backup again.</li>
</ol>
<h2 id="icloud-account-migration">iCloud Account Migration</h2>
<p>Suppose that you cannot take back the iCloud account, you will need to migrate to a new iCloud account.</p>
<h3 id="disabling-mobile-data-to-prevent-accidental-usage">Disabling Mobile Data to Prevent Accidental Usage</h3>
<ol>
<li>Swipe from the bottom of the screen up to show the control center.</li>
<li>Tap the (green) tower icon.</li>
</ol>
<h3 id="sign-out-of-the-old-apple-id">Sign out of the Old Apple ID</h3>
<p>If your data is very important, you should consider testing your iTunes backups by restorting them to another phone first. A backup that cannot be restored is not a backup.</p>
<ol>
<li>Open Settings.</li>
<li>Tap your Apple ID.</li>
<li>Scroll down to tap Sign Out.</li>
<li>Enable keeping data for all entries in the prompt.</li>
</ol>
<h3 id="sign-in-of-the-new-apple-id">Sign in of the New Apple ID</h3>
<ol>
<li>Open Settings.</li>
<li>Tap Apple ID.</li>
<li>Fill in the email address and password.</li>
<li>Select merge when you are asked to merge the data.</li>
<li>Enable appropriate apps under Settings → Apple ID → iCloud → APPS USING ICLOUD.</li>
<li>Wait for the synchronization to complete.</li>
</ol>
<h2 id="clean-up">Clean up</h2>
<ol>
<li>Re-enable mobile data.</li>
<li>Disable password-protected iTunes backup if it is enabled in the previous steps.</li>
<li>Delete the iTunes backups.</li>
<li>Delete the manual photo backup.</li>
</ol>
<h2 id="strengthening-security">Strengthening Security</h2>
<h3 id="disabling-sms-on-the-lock-screen">Disabling SMS on the Lock Screen</h3>
<p>Option 1: disabling previews</p>
<ol>
<li>Open Settings.</li>
<li>Tap Notifications.</li>
<li>Tap Messages.</li>
<li>Select Never for Show Previews.</li>
</ol>
<p>Option 2: disabling previews on the lock screen only</p>
<ol>
<li>Open Settings.</li>
<li>Tap Notifications.</li>
<li>Tap Messages.</li>
<li>Deselect Lock Screen.</li>
</ol>
<h2 id="disabling-fingerprint-unlock">Disabling Fingerprint Unlock</h2>
<ol>
<li>Open Settings.</li>
<li>Scroll down to and tap Touch ID &amp; Passcode.</li>
<li>Disable iPhone Unlock under USE TOUCH ID FOR.</li>
</ol>
<h2 id="enabling-find-my-phone-icloud-lock">Enabling Find My Phone (iCloud Lock)</h2>
<ol>
<li>Open Settings.</li>
<li>Tap your Apple ID.</li>
<li>Tap Find My.</li>
<li>Tap Find My Phone.</li>
<li>Switch on Find My Phone.</li>
</ol>
<h1 id="routine-maintenance">Routine Maintenance</h1>
<p>The following items should be checked or performed periodically.</p>
<h2 id="disabling-sms-on-the-lock-screen-1">Disabling SMS on The Lock Screen</h2>
<p>Option 1: disabling previews</p>
<ol>
<li>Open Settings.</li>
<li>Tap Notifications.</li>
<li>Tap Messages.</li>
<li>Select Never for Show Previews.</li>
</ol>
<p>Option 2: disabling previews on the lock screen only</p>
<ol>
<li>Open Settings.</li>
<li>Tap Notifications.</li>
<li>Tap Messages.</li>
<li>Deselect Lock Screen.</li>
</ol>
<h2 id="disabling-fingerprint-lock">Disabling Fingerprint Lock</h2>
<ol>
<li>Open Settings.</li>
<li>Scroll down to and tap Touch ID &amp; Passcode.</li>
<li>Disable iPhone Unlock under USE TOUCH ID FOR.</li>
</ol>
<h2 id="enabling-find-my-phone-icloud-lock-1">Enabling Find My Phone (iCloud Lock)</h2>
<ol>
<li>Open Settings.</li>
<li>Tap your Apple ID.</li>
<li>Tap Find My.</li>
<li>Tap Find My Phone.</li>
<li>Switch on Find My Phone.</li>
</ol>
<h2 id="checking-whatsapp-login-activity">Checking Whatsapp Login Activity</h2>
<ol>
<li>Open Whatsapp.</li>
<li>Open Settings.</li>
<li>Tap WhatsApp Web/Desktop.</li>
<li>Being asked to scan a QR code means that there are currently no other logins.</li>
</ol>
<h2 id="google-account-security">Google Account Security</h2>
<p>Check for no suspicious logins, extra 2-step verification methods, and password recovery methods.</p>
<ol>
<li>Log in to <a href="https://www.google.com">https://www.google.com</a> in a browser with the password to verify if it was unchanged.</li>
<li>Click the account icon.</li>
<li>Click Manage your Google A/C.</li>
<li>Click Security.</li>
<li>Under Signing in to Google, 2-Step Verification, there are
<ol>
<li>no Authenticator app</li>
<li>no Backup codes</li>
<li>no Google prompts</li>
<li>only the correct phone number in Voice or text message</li>
<li>no Security Key</li>
<li>no Devices you trust</li>
</ol>
</li>
<li>Under Signing in to Google, there are no entries in App passwords.</li>
<li>Under Ways we can verify it's you, only the correct phone number in recovery phones and no recovery emails.</li>
<li>Under Recent security activity, there are no unknown logins.</li>
<li>Under Your devices, there are no other devices.</li>
<li>Under Signing in to other sites, there are no entries in Third-party apps with account access and each category.</li>
</ol>
<h2 id="apple-id">Apple ID</h2>
<ol>
<li>Log in to <a href="https://appleid.apple.com">https://appleid.apple.com</a> in a browser with the password to verify if it was unchanged.</li>
<li>Click the edit button in the security section.</li>
<li>There is only the correct phone number under TRUSTED PHONE NUMBERS.</li>
<li>There is no view history under APP-SPECIFIC PASSWORDS.</li>
<li>TWO-FACTOR AUTHENTICATION is on.</li>
<li>There is only the phone in Devices.</li>
</ol>
<h1 id="execution">Execution</h1>
<p>The process took only 3 - 4 hours, greatly thanks to friend D being an above-average computer user and an extra phone for showing me their screens. We spent quite a substantial amount of time trying to take the accounts back. We could not take Billy's Microsoft email account back because of not knowing the password and answers to the security questions. We applied for a review of our declaration of our ownership of the account. We could not take iCloud account back too so we did an iCloud migration. There were only small deviations from the plan which they were easy to deal with. There was time for me to educate Billy on how to protect their digital privacy.</p>
<h1 id="result">Result</h1>
<p>A few days later, Alex sneakily attempted to take Billy's phone and acted suspiciously. This was a sign of temporary success. However, I suspected that Alex knew the password of the phone. Billy ensured that Alex did not know the password of the phone. It was a stressful enough situation so I did not push Billy to use a new password.</p>
<p>We did not receive any responses from Microsoft so the email account was lost forever. Billy would notify their friends of the new email address in person. To ensure the security would hold, friend D would help Billy to perform the routine maintenance. That was the best we could do for the moment.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The procedure was lengthy and might seem complicated to most people. It was harder to defend than attack. One did not know how their adversaries would attack so the defense had to thoroughly cover all angles. The procedure was slightly different from the best practice for digital security. It was required to suit the rather unusual situation. It was unfortunate that we could not take back all accounts.</p>]]></content>
  </entry>
</feed>
